{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2XorbcBoK70",
        "outputId": "fc49e0d3-d6c5-4865-c5a7-dc63dcc6c2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Starting to scrape MS in Applied Data Science program information ====\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/ (Type: main)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 8 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/ (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 48 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/capstone-projects/ (Type: project)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 14 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/course-progressions/ (Type: curriculum)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 77 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/how-to-apply/ (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 11 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/events-deadlines/ (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 8 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/our-students/ (Type: student)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/instructors-staff/ (Type: faculty)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 4 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/ (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 43 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/career-outcomes/ (Type: career)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-0 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 36 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-1 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 31 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-2 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 26 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-3 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 21 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-4 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 16 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-5 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 13 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-6 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 11 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-7 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 10 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-8 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 9 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-9 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 8 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-10 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 7 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-11 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-12 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 5 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-13 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 4 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-14 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 3 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-15 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 2 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-16 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 1 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faqs/#accordion-tab-17 (Type: faq)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/course-progressions/#accordion-tab-0 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/course-progressions/#accordion-tab-1 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/ (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 27 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-0 (Type: career)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 24 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-1 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 21 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-2 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 18 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-3 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 15 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-4 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 12 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-5 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 9 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-6 (Type: project)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 7 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-7 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-8 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 5 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-9 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 4 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-10 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 3 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-11 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 2 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-12 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 1 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/#accordion-tab-13 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/tuition-fees-aid/ (Type: tuition)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-0 (Type: career)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 27 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-1 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 24 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-2 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 21 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-3 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 18 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-4 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 15 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-5 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 12 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-6 (Type: project)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 10 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-7 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 9 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-8 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 8 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-9 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 7 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-10 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-11 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 5 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-12 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 4 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-13 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 3 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-14 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 2 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-15 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 1 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/in-person-program/#accordion-tab-16 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 27 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-0 (Type: career)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 24 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-1 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 21 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-2 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 18 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-3 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 15 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-4 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 12 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-5 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 9 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-6 (Type: project)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 7 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-7 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 6 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-8 (Type: admission)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 5 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-9 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 4 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-10 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 3 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-11 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 2 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-12 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 1 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/online-program/%20#accordion-tab-13 (Type: general)\n",
            "  ✓ Content successfully extracted\n",
            "  Found 0 related links\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/curriculum/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/curriculum/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/admission/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/admission/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faculty/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faculty/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/tuition-and-aid/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/tuition-and-aid/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/careers/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/careers/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faq/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/faq/: Status code 404\n",
            "Scraping: https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/student-experience/ (Type: main)\n",
            "Failed to retrieve https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/student-experience/: Status code 404\n",
            "\n",
            "==== Scraping Statistics ====\n",
            "Total URLs visited: 85\n",
            "Successfully extracted content items: 78\n",
            "\n",
            "Page type distribution:\n",
            "  - main: 1 pages\n",
            "  - general: 39 pages\n",
            "  - project: 4 pages\n",
            "  - curriculum: 1 pages\n",
            "  - admission: 14 pages\n",
            "  - student: 1 pages\n",
            "  - faculty: 1 pages\n",
            "  - faq: 12 pages\n",
            "  - career: 4 pages\n",
            "  - tuition: 1 pages\n",
            "\n",
            "Data saved to data/ms_applied_data_science_data.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class MSDataScienceFocusedScraper:\n",
        "    \"\"\"Optimized web scraper for MS in Applied Data Science program\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/\"\n",
        "        self.target_path = \"/education/masters-programs/ms-in-applied-data-science/\"\n",
        "        self.visited_urls = set()\n",
        "        self.data = []\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        # Define important URLs to ensure these pages are scraped\n",
        "        self.important_urls = [\n",
        "            self.base_url,                                      # Main page\n",
        "            urljoin(self.base_url, \"curriculum/\"),              # Curriculum\n",
        "            urljoin(self.base_url, \"admission/\"),               # Admission requirements\n",
        "            urljoin(self.base_url, \"faculty/\"),                 # Faculty information\n",
        "            urljoin(self.base_url, \"tuition-and-aid/\"),         # Tuition and financial aid\n",
        "            urljoin(self.base_url, \"careers/\"),                 # Career outcomes\n",
        "            urljoin(self.base_url, \"capstone-projects/\"),       # Capstone projects\n",
        "            urljoin(self.base_url, \"faq/\"),                     # Frequently asked questions\n",
        "            urljoin(self.base_url, \"student-experience/\")       # Student experience\n",
        "        ]\n",
        "\n",
        "    def start_scraping(self):\n",
        "        \"\"\"Start the scraping process, beginning with important pages\"\"\"\n",
        "        print(\"==== Starting to scrape MS in Applied Data Science program information ====\")\n",
        "\n",
        "        # First scrape the known important URLs\n",
        "        for url in self.important_urls:\n",
        "            # Try to access the page, continue even if URL doesn't exist\n",
        "            try:\n",
        "                page_type = self.determine_page_type_from_url(url)\n",
        "                self.scrape_page(url, page_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Important page {url} scraping failed: {str(e)}\")\n",
        "\n",
        "        # Then start from main page to discover additional related pages\n",
        "        if self.base_url not in self.visited_urls:\n",
        "            self.scrape_page(self.base_url, \"main\")\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def scrape_page(self, url, page_type):\n",
        "        \"\"\"Scrape a single page and extract its content\"\"\"\n",
        "        if url in self.visited_urls:\n",
        "            return\n",
        "\n",
        "        # Check if URL belongs to the target path\n",
        "        parsed_url = urlparse(url)\n",
        "        if self.target_path not in parsed_url.path:\n",
        "            return\n",
        "\n",
        "        print(f\"Scraping: {url} (Type: {page_type})\")\n",
        "        self.visited_urls.add(url)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Failed to retrieve {url}: Status code {response.status_code}\")\n",
        "                return\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # --- MODIFIED: Pass 'url' as the base_url for link resolution ---\n",
        "            content = self.extract_content(soup, page_type, url)\n",
        "\n",
        "            if content:\n",
        "                self.data.append({\n",
        "                    \"url\": url,\n",
        "                    \"page_type\": page_type,\n",
        "                    \"title\": self.extract_title(soup),\n",
        "                    \"content\": content,\n",
        "                    \"last_scraped\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                })\n",
        "                print(f\"  ✓ Content successfully extracted\")\n",
        "            else:\n",
        "                print(f\"  ✗ Failed to extract content\")\n",
        "\n",
        "            # Find other related links and recursively scrape them\n",
        "            related_links = self.find_related_links(soup, url)\n",
        "            print(f\"  Found {len(related_links)} related links\")\n",
        "\n",
        "            for link_url, link_type in related_links:\n",
        "                self.scrape_page(link_url, link_type)\n",
        "\n",
        "            # Polite delay between requests\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {str(e)}\")\n",
        "\n",
        "    # --- NEW HELPER FUNCTION ---\n",
        "    def _resolve_links_in_tag(self, tag, base_url):\n",
        "        \"\"\"\n",
        "        Find all <a> tags within a given BeautifulSoup tag and\n",
        "        modify their href attribute to be an absolute URL.\n",
        "        \"\"\"\n",
        "        if not tag:\n",
        "            return\n",
        "\n",
        "        for a_tag in tag.find_all('a', href=True):\n",
        "            href = a_tag.get('href')\n",
        "\n",
        "            # Skip javascript: or mailto: links\n",
        "            if not href or href.startswith('javascript:') or href.startswith('mailto:'):\n",
        "                continue\n",
        "\n",
        "            # Use urljoin to create the absolute URL\n",
        "            # This correctly handles:\n",
        "            # 1. href=\"#FeeWaiver\"\n",
        "            # 2. href=\"/admissions/\"\n",
        "            # 3. href=\"https.other.site/page.html\"\n",
        "            absolute_href = urljoin(base_url, href)\n",
        "\n",
        "            # Modify the tag's href attribute in-place\n",
        "            a_tag['href'] = absolute_href\n",
        "\n",
        "    def extract_title(self, soup):\n",
        "        \"\"\"Extract the page title using different methods\"\"\"\n",
        "        # (This function is unchanged)\n",
        "        title = soup.find('h1')\n",
        "        if title and title.text.strip():\n",
        "            return title.text.strip()\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag and title_tag.text.strip():\n",
        "            title_text = title_tag.text.strip()\n",
        "            if \" | \" in title_text:\n",
        "                title_text = title_text.split(\" | \")[0]\n",
        "            return title_text\n",
        "        for tag in ['h2', 'h3']:\n",
        "            title = soup.find(tag)\n",
        "            if title and title.text.strip():\n",
        "                return title.text.strip()\n",
        "        return \"Unknown Title\"\n",
        "\n",
        "    # --- MODIFIED: Accept and use base_url ---\n",
        "    def extract_content(self, soup, page_type, base_url):\n",
        "        \"\"\"Extract content based on page type with specialized extraction\"\"\"\n",
        "        main_content = soup.find('main') or soup.find('div', class_='content-area') or soup.find('article') or soup\n",
        "        content = {}\n",
        "\n",
        "        # Extract paragraphs (as HTML strings)\n",
        "        paragraphs = main_content.find_all('p')\n",
        "        for p in paragraphs:\n",
        "            self._resolve_links_in_tag(p, base_url) # Resolve links before saving\n",
        "        content['paragraphs'] = [str(p) for p in paragraphs if p.text.strip()]\n",
        "\n",
        "        # Extract headings (as text, links unlikely)\n",
        "        headings = main_content.find_all(['h2', 'h3', 'h4'])\n",
        "        content['headings'] = [h.text.strip() for h in headings if h.text.strip()]\n",
        "\n",
        "        # Extract lists (as HTML strings)\n",
        "        lists = main_content.find_all(['ul', 'ol'])\n",
        "        content['lists'] = []\n",
        "        for lst in lists:\n",
        "            items = lst.find_all('li')\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    self._resolve_links_in_tag(item, base_url) # Resolve links\n",
        "                content['lists'].append([str(item) for item in items if item.text.strip()])\n",
        "\n",
        "        # Extract tables (as HTML strings)\n",
        "        tables = main_content.find_all('table')\n",
        "        content['tables'] = []\n",
        "        for table in tables:\n",
        "            rows = table.find_all('tr')\n",
        "            if rows:\n",
        "                table_data = []\n",
        "                for row in rows:\n",
        "                    cells = row.find_all(['td', 'th'])\n",
        "                    if cells:\n",
        "                        for cell in cells:\n",
        "                            self._resolve_links_in_tag(cell, base_url) # Resolve links\n",
        "                        table_data.append([str(cell) for cell in cells])\n",
        "                content['tables'].append(table_data)\n",
        "\n",
        "        # Extract page-specific content based on page type\n",
        "        if page_type == \"curriculum\":\n",
        "            courses = self.extract_courses(main_content, base_url) # Pass base_url\n",
        "            if courses:\n",
        "                content['courses'] = courses\n",
        "\n",
        "        elif page_type == \"admission\":\n",
        "            requirements = self.extract_requirements(main_content, base_url) # Pass base_url\n",
        "            if requirements:\n",
        "                content['requirements'] = requirements\n",
        "\n",
        "        elif page_type == \"tuition\":\n",
        "            tuition_info = self.extract_tuition(main_content, base_url) # Pass base_url\n",
        "            if tuition_info:\n",
        "                content['tuition_info'] = tuition_info\n",
        "\n",
        "        elif page_type == \"faculty\":\n",
        "            faculty = self.extract_faculty(main_content, base_url) # Pass base_url\n",
        "            if faculty:\n",
        "                content['faculty'] = faculty\n",
        "\n",
        "        # Extract any button text and links on the page\n",
        "        buttons = main_content.find_all(['a', 'button'], class_=lambda c: c and ('btn' in c or 'button' in c))\n",
        "        if buttons:\n",
        "            content['buttons'] = []\n",
        "            for button in buttons:\n",
        "                button_info = {\n",
        "                    'text': button.text.strip()\n",
        "                }\n",
        "                if button.name == 'a' and button.get('href'):\n",
        "                    # --- FIXED: Resolve button links too ---\n",
        "                    button_info['link'] = urljoin(base_url, button['href'])\n",
        "                content['buttons'].append(button_info)\n",
        "\n",
        "        has_content = False\n",
        "        for key, value in content.items():\n",
        "            if value:\n",
        "                has_content = True\n",
        "                break\n",
        "        return content if has_content else None\n",
        "\n",
        "    # --- MODIFIED: Accept and use base_url ---\n",
        "    def extract_courses(self, content_area, base_url):\n",
        "        \"\"\"Extract course information from the curriculum page\"\"\"\n",
        "        courses = []\n",
        "        course_sections = content_area.find_all('div', class_=lambda c: c and ('course' in c.lower()))\n",
        "        if not course_sections:\n",
        "            course_headers = content_area.find_all(['h2', 'h3', 'h4'], string=lambda s: s and ('course' in s.lower() or 'curriculum' in s.lower()))\n",
        "            for header in course_headers:\n",
        "                next_elements = list(header.next_siblings)\n",
        "                if next_elements:\n",
        "                    course_sections.append(header.parent)\n",
        "        if course_sections:\n",
        "            for section in course_sections:\n",
        "                title_elem = section.find(['h3', 'h4', 'h5', 'strong'])\n",
        "                title = title_elem.text.strip() if title_elem else \"Unknown Course\"\n",
        "                desc_elem = section.find('p')\n",
        "                self._resolve_links_in_tag(desc_elem, base_url) # Resolve links\n",
        "                description = str(desc_elem) if desc_elem else \"\"\n",
        "                course = {'title': title, 'description': description}\n",
        "                list_items = section.find_all('li')\n",
        "                if list_items:\n",
        "                    for item in list_items:\n",
        "                        self._resolve_links_in_tag(item, base_url) # Resolve links\n",
        "                    details = [str(item) for item in list_items]\n",
        "                    course['details'] = details\n",
        "                courses.append(course)\n",
        "        if not courses:\n",
        "            course_lists = content_area.find_all('ul')\n",
        "            for ul in course_lists:\n",
        "                items = ul.find_all('li')\n",
        "                if len(items) > 3:\n",
        "                    text = ul.text.lower()\n",
        "                    if 'course' in text or 'curriculum' in text or 'class' in text:\n",
        "                        for item in items:\n",
        "                            self._resolve_links_in_tag(item, base_url) # Resolve links\n",
        "                            courses.append({\n",
        "                                'title': item.text.strip(),\n",
        "                                'description': \"\",\n",
        "                                'details': [str(item)]\n",
        "                            })\n",
        "                        break\n",
        "        return courses\n",
        "\n",
        "    # --- MODIFIED: Accept and use base_url ---\n",
        "    def extract_requirements(self, content_area, base_url):\n",
        "        \"\"\"Extract admission requirements information\"\"\"\n",
        "        requirements = {}\n",
        "        req_headers = content_area.find_all(['h2', 'h3', 'h4'], string=lambda s: s and any(word in s.lower() for word in ['requirement', 'prerequisite', 'application', 'admission']))\n",
        "        for header in req_headers:\n",
        "            section_title = header.text.strip()\n",
        "            section_content = []\n",
        "            current = header.next_sibling\n",
        "            while current and current.name != header.name:\n",
        "                if current.name == 'p' and current.text.strip():\n",
        "                    self._resolve_links_in_tag(current, base_url) # Resolve links\n",
        "                    section_content.append(str(current))\n",
        "                elif current.name in ['ul', 'ol']:\n",
        "                    items = current.find_all('li')\n",
        "                    for item in items:\n",
        "                        self._resolve_links_in_tag(item, base_url) # Resolve links\n",
        "                    section_content.extend([str(item) for item in items if item.text.strip()])\n",
        "                current = current.next_sibling\n",
        "            if section_content:\n",
        "                requirements[section_title] = section_content\n",
        "        if not requirements:\n",
        "            relevant_paras = []\n",
        "            for p in content_area.find_all('p'):\n",
        "                text = p.text.lower()\n",
        "                if any(word in text for word in ['requirement', 'prerequisite', 'application', 'admission', 'apply', 'gre', 'toefl']):\n",
        "                    self._resolve_links_in_tag(p, base_url) # Resolve links\n",
        "                    relevant_paras.append(str(p))\n",
        "            if relevant_paras:\n",
        "                requirements['General Requirements'] = relevant_paras\n",
        "        return requirements\n",
        "\n",
        "    # --- MODIFIED: Accept and use base_url ---\n",
        "    def extract_tuition(self, content_area, base_url):\n",
        "        \"\"\"Extract tuition and financial aid information\"\"\"\n",
        "        tuition_info = {}\n",
        "        tables = content_area.find_all('table')\n",
        "        for table in tables:\n",
        "            table_text = table.text.lower()\n",
        "            if 'tuition' in table_text or 'fee' in table_text or 'cost' in table_text:\n",
        "                rows = table.find_all('tr')\n",
        "                tuition_table = []\n",
        "                for row in rows:\n",
        "                    cells = row.find_all(['td', 'th'])\n",
        "                    if cells:\n",
        "                        for cell in cells:\n",
        "                            self._resolve_links_in_tag(cell, base_url) # Resolve links\n",
        "                        tuition_table.append([str(cell) for cell in cells])\n",
        "                if tuition_table:\n",
        "                    tuition_info['tuition_table'] = tuition_table\n",
        "        tuition_paras = []\n",
        "        for p in content_area.find_all('p'):\n",
        "            text = p.text.lower()\n",
        "            if 'tuition' in text or 'fee' in text or 'cost' in text or 'financial' in text or 'scholarship' in text:\n",
        "                self._resolve_links_in_tag(p, base_url) # Resolve links\n",
        "                tuition_paras.append(str(p))\n",
        "        if tuition_paras:\n",
        "            tuition_info['tuition_text'] = tuition_paras\n",
        "        return tuition_info\n",
        "\n",
        "    # --- MODIFIED: Accept and use base_url ---\n",
        "    def extract_faculty(self, content_area, base_url):\n",
        "        \"\"\"Extract faculty information\"\"\"\n",
        "        faculty = []\n",
        "        faculty_sections = content_area.find_all('div', class_=lambda c: c and any(word in c.lower() for word in ['faculty', 'instructor', 'professor', 'staff']))\n",
        "        for section in faculty_sections:\n",
        "            name_elem = section.find(['h3', 'h4', 'h5', 'strong'])\n",
        "            if not name_elem:\n",
        "                continue\n",
        "            faculty_member = {'name': name_elem.text.strip()}\n",
        "            title_elem = name_elem.find_next('p')\n",
        "            if title_elem:\n",
        "                faculty_member['title'] = title_elem.text.strip()\n",
        "            bio = []\n",
        "            current = title_elem.next_sibling if title_elem else name_elem.next_sibling\n",
        "            while current and current.name not in ['h3', 'h4', 'h5', 'strong']:\n",
        "                if current.name == 'p' and current.text.strip():\n",
        "                    self._resolve_links_in_tag(current, base_url) # Resolve links\n",
        "                    bio.append(str(current))\n",
        "                current = current.next_sibling\n",
        "            if bio:\n",
        "                faculty_member['bio'] = bio\n",
        "            faculty.append(faculty_member)\n",
        "        if not faculty:\n",
        "            faculty_lists = content_area.find_all('ul')\n",
        "            for ul in faculty_lists:\n",
        "                if 'faculty' in ul.text.lower() or 'instructor' in ul.text.lower() or 'professor' in ul.text.lower():\n",
        "                    items = ul.find_all('li')\n",
        "                    for item in items:\n",
        "                        self._resolve_links_in_tag(item, base_url) # Resolve links\n",
        "                        faculty.append({\n",
        "                            'name': item.text.strip(),\n",
        "                            'bio': [str(item)]\n",
        "                        })\n",
        "                    break\n",
        "        return faculty\n",
        "\n",
        "    def find_related_links(self, soup, current_url):\n",
        "        \"\"\"Find related links on the page that belong to the same project\"\"\"\n",
        "        # (This function is unchanged)\n",
        "        related_links = []\n",
        "        domain = \"datascience.uchicago.edu\"\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            if not href or href.startswith('#') or href.startswith('javascript:'):\n",
        "                continue\n",
        "            full_url = urljoin(current_url, href)\n",
        "            if full_url in self.visited_urls or domain not in full_url:\n",
        "                continue\n",
        "            parsed_url = urlparse(full_url)\n",
        "            if self.target_path in parsed_url.path:\n",
        "                page_type = self.determine_page_type(parsed_url.path, a_tag.text)\n",
        "                related_links.append((full_url, page_type))\n",
        "        return related_links\n",
        "\n",
        "    def determine_page_type(self, path, link_text):\n",
        "        \"\"\"Determine page type based on URL path and link text\"\"\"\n",
        "        # (This function is unchanged)\n",
        "        path = path.lower()\n",
        "        link_text = link_text.lower() if link_text else \"\"\n",
        "        if \"/curriculum\" in path or any(word in link_text for word in [\"course\", \"curriculum\", \"class\", \"program structure\"]):\n",
        "            return \"curriculum\"\n",
        "        elif \"/admission\" in path or any(word in link_text for word in [\"apply\", \"application\", \"admission\", \"requirements\"]):\n",
        "            return \"admission\"\n",
        "        elif \"/faculty\" in path or any(word in link_text for word in [\"faculty\", \"professor\", \"instructor\", \"teacher\", \"staff\"]):\n",
        "            return \"faculty\"\n",
        "        elif \"/tuition\" in path or \"/cost\" in path or \"/aid\" in path or any(word in link_text for word in [\"tuition\", \"cost\", \"fee\", \"financial aid\", \"scholarship\"]):\n",
        "            return \"tuition\"\n",
        "        elif \"/career\" in path or \"/job\" in path or any(word in link_text for word in [\"career\", \"job\", \"employment\", \"placement\", \"alumni\"]):\n",
        "            return \"career\"\n",
        "        elif \"/capstone\" in path or \"/project\" in path or any(word in link_text for word in [\"capstone\", \"project\", \"portfolio\"]):\n",
        "            return \"project\"\n",
        "        elif \"/faq\" in path or any(word in link_text for word in [\"faq\", \"question\", \"answer\"]):\n",
        "            return \"faq\"\n",
        "        elif \"/student\" in path or \"/experience\" in path or any(word in link_text for word in [\"student\", \"experience\", \"life\", \"testimonial\"]):\n",
        "            return \"student\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def determine_page_type_from_url(self, url):\n",
        "        \"\"\"Determine page type directly from URL, used for important URLs\"\"\"\n",
        "        # (This function is unchanged)\n",
        "        path = urlparse(url).path.lower()\n",
        "        if path == self.target_path or path.endswith('/'):\n",
        "            return \"main\"\n",
        "        elif \"/curriculum\" in path:\n",
        "            return \"curriculum\"\n",
        "        elif \"/admission\" in path:\n",
        "            return \"admission\"\n",
        "        elif \"/faculty\" in path:\n",
        "            return \"faculty\"\n",
        "        elif \"/tuition\" in path or \"/aid\" in path:\n",
        "            return \"tuition\"\n",
        "        elif \"/career\" in path:\n",
        "            return \"career\"\n",
        "        elif \"/capstone\" in path or \"/project\" in path:\n",
        "            return \"project\"\n",
        "        elif \"/faq\" in path:\n",
        "            return \"faq\"\n",
        "        elif \"/student\" in path or \"/experience\" in path:\n",
        "            return \"student\"\n",
        "        else:\n",
        "            return \"general\"\n",
        "\n",
        "    def save_data(self, filename):\n",
        "        \"\"\"Save data to file with detailed statistics\"\"\"\n",
        "        # (This function is unchanged)\n",
        "        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.data, f, ensure_ascii=False, indent=2)\n",
        "        page_type_counts = {}\n",
        "        for item in self.data:\n",
        "            page_type = item['page_type']\n",
        "            page_type_counts[page_type] = page_type_counts.get(page_type, 0) + 1\n",
        "        print(\"\\n==== Scraping Statistics ====\")\n",
        "        print(f\"Total URLs visited: {len(self.visited_urls)}\")\n",
        "        print(f\"Successfully extracted content items: {len(self.data)}\")\n",
        "        print(\"\\nPage type distribution:\")\n",
        "        for page_type, count in page_type_counts.items():\n",
        "            print(f\"  - {page_type}: {count} pages\")\n",
        "        print(f\"\\nData saved to {filename}\")\n",
        "\n",
        "# Execute scraping\n",
        "if __name__ == \"__main__\":\n",
        "    # (This section is unchanged)\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    scraper = MSDataScienceFocusedScraper()\n",
        "    data = scraper.start_scraping()\n",
        "    scraper.save_data(\"data/ms_applied_data_science_data.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0dGypesoTzS",
        "outputId": "593303bd-313a-41d8-c8a1-c1ac0c6da0cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/alexlee/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/alexlee/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/alexlee/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/alexlee/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting enhanced data preprocessing...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 473\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m EnhancedDataPreprocessor(input_data_path)\n\u001b[0;32m--> 473\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# --- THIS IS THE LINE I FIXED (I added the parentheses) ---\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunks:\n",
            "Cell \u001b[0;32mIn[19], line 109\u001b[0m, in \u001b[0;36mEnhancedDataPreprocessor.preprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Process each scraped item\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data:\n\u001b[0;32m--> 109\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks\u001b[38;5;241m.\u001b[39mextend(chunks)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing complete. Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m text chunks with semantic tags and keywords.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[19], line 138\u001b[0m, in \u001b[0;36mEnhancedDataPreprocessor.process_item\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    137\u001b[0m             keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_keywords(text)\n\u001b[0;32m--> 138\u001b[0m             semantic_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_semantic_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m             chunks\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    141\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 }\n\u001b[1;32m    149\u001b[0m             })\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Process lists\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[19], line 382\u001b[0m, in \u001b[0;36mEnhancedDataPreprocessor.extract_semantic_tags\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract semantic tags including dates, numbers, and special information types\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m semantic_tags \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 382\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m dates \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    384\u001b[0m date_patterns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb(January|February|March|April|May|June|July|August|September|October|November|December)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}(?:st|nd|rd|th)?,\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,4}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbdue\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+date\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,4}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    393\u001b[0m ]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/language.py:1053\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py:130\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    128\u001b[0m error_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations(doc, matches)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/pipeline/attributeruler.py:137\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: Doc):\n\u001b[0;32m--> 137\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_spans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     matches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m         (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[38;5;28;01mfor\u001b[39;00m m_id, s, e \u001b[38;5;129;01min\u001b[39;00m matches  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import urllib.parse  # <-- This import is the key to fixing links\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup # Import BeautifulSoup\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:  # Catch the specific error\n",
        "    print(\"Model 'en_core_web_sm' not found. Downloading...\")\n",
        "    import subprocess\n",
        "\n",
        "    # Use sys.executable to ensure we're using the correct python interpreter\n",
        "    subprocess.call([\n",
        "        sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"\n",
        "    ])\n",
        "\n",
        "    # After download, import the package directly and load it\n",
        "    try:\n",
        "        import en_core_web_sm\n",
        "        nlp = en_core_web_sm.load()\n",
        "    except ImportError:\n",
        "        print(\"\\n---\")\n",
        "        print(\"Failed to load model even after download.\")\n",
        "        print(\"Please try running this command in your terminal:\")\n",
        "        print(f\"   {sys.executable} -m spacy download en_core_web_sm\")\n",
        "        print(\"Then, restart the script.\")\n",
        "        print(\"---\")\n",
        "        exit() # Exit the script if the model can't be loaded\n",
        "\n",
        "\n",
        "class EnhancedDataPreprocessor:\n",
        "    \"\"\"Enhanced data preprocessor with semantic tagging and keyword extraction\"\"\"\n",
        "\n",
        "    def __init__(self, input_file: str):\n",
        "        \"\"\"Initialize with path to scraped data file\"\"\"\n",
        "        self.input_file = input_file\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            self.raw_data = json.load(f)\n",
        "\n",
        "        self.processed_chunks = []\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "        self.domain_keywords = {\n",
        "            'education': ['program', 'course', 'curriculum', 'study', 'learn', 'academic', 'education', 'degree', 'student', 'faculty'],\n",
        "            'application': ['apply', 'admission', 'requirement', 'deadline', 'application', 'submit', 'gre', 'toefl', 'ielts', 'recommendation'],\n",
        "            'career': ['job', 'career', 'employment', 'industry', 'professional', 'opportunity', 'salary', 'placement', 'alumni', 'hire'],\n",
        "            'financial': ['tuition', 'fee', 'cost', 'financial', 'aid', 'scholarship', 'funding', 'loan', 'payment', 'assistantship']\n",
        "        }\n",
        "\n",
        "    # --- THIS IS THE FUNCTION THAT FIXES THE LINKS ---\n",
        "    def _extract_text_and_links(self, html_string: str, base_url: str) -> Tuple[str, List[Dict[str, str]]]:\n",
        "        \"\"\"Helper to get clean text and all embedded links from an HTML string.\"\"\"\n",
        "        if not html_string:\n",
        "            return \"\", []\n",
        "\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_string, 'html.parser')\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            links = []\n",
        "            seen_hrefs = set()\n",
        "\n",
        "            for a_tag in soup.find_all('a', href=True):\n",
        "                relative_href = a_tag['href']\n",
        "\n",
        "                # Use urljoin to build the full, absolute URL.\n",
        "                # This correctly handles:\n",
        "                # 1. href=\"#FeeWaiver\"\n",
        "                # 2. href=\"/admissions/...\"\n",
        "                # 3. href=\"https://...#FeeWaiver\"\n",
        "                absolute_href = urllib.parse.urljoin(base_url, relative_href)\n",
        "\n",
        "                href = absolute_href\n",
        "\n",
        "                if href not in seen_hrefs:\n",
        "                    links.append({\n",
        "                        'text': a_tag.get_text(strip=True),\n",
        "                        'href': href\n",
        "                    })\n",
        "                    seen_hrefs.add(href)\n",
        "\n",
        "            return text, links\n",
        "        except Exception:\n",
        "            # Fallback for plain text if HTML parsing fails\n",
        "            return html_string, []\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"Main preprocessing function with enhanced features\"\"\"\n",
        "        print(\"Starting enhanced data preprocessing...\")\n",
        "\n",
        "        # Process each scraped item\n",
        "        for item in self.raw_data:\n",
        "            chunks = self.process_item(item)\n",
        "            self.processed_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"Preprocessing complete. Created {len(self.processed_chunks)} text chunks with semantic tags and keywords.\")\n",
        "        return self.processed_chunks\n",
        "\n",
        "    # --- THIS FUNCTION PASSES THE base_url ---\n",
        "    def process_item(self, item: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process a single scraped item into multiple text chunks with enhanced metadata\"\"\"\n",
        "        chunks = []\n",
        "        content = item['content']\n",
        "        base_url = item['url']  # This is the base URL for resolving relative links\n",
        "\n",
        "        # Create basic metadata for all chunks from this item\n",
        "        metadata = {\n",
        "            \"source\": base_url,\n",
        "            \"title\": item['title'],\n",
        "            \"page_type\": item['page_type'],\n",
        "            \"last_scraped\": item.get('last_scraped', '')\n",
        "        }\n",
        "\n",
        "        # Process paragraphs\n",
        "        if 'paragraphs' in content and content['paragraphs']:\n",
        "            for paragraph_html in content['paragraphs']:\n",
        "                # Pass base_url to the helper function\n",
        "                text, links = self._extract_text_and_links(paragraph_html, base_url)\n",
        "\n",
        "                if text and len(text.split()) >= 10:\n",
        "                    keywords = self.extract_keywords(text)\n",
        "                    semantic_tags = self.extract_semantic_tags(text)\n",
        "\n",
        "                    chunks.append({\n",
        "                        \"text\": text,\n",
        "                        \"metadata\": {\n",
        "                            **metadata,\n",
        "                            \"content_type\": \"paragraph\",\n",
        "                            \"keywords\": keywords,\n",
        "                            \"semantic_tags\": semantic_tags,\n",
        "                            \"embedded_links\": links\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "        # Process lists\n",
        "        if 'lists' in content and content['lists']:\n",
        "            for list_items_html in content['lists']:\n",
        "                if list_items_html:\n",
        "                    text_parts = []\n",
        "                    all_links = []\n",
        "\n",
        "                    for item_html in list_items_html:\n",
        "                        # Pass base_url to the helper function\n",
        "                        text, links = self._extract_text_and_links(item_html, base_url)\n",
        "                        text_parts.append(text)\n",
        "                        all_links.extend(links)\n",
        "\n",
        "                    list_text = \"\\n• \" + \"\\n• \".join(text_parts)\n",
        "                    keywords = self.extract_keywords(list_text)\n",
        "                    semantic_tags = self.extract_semantic_tags(list_text)\n",
        "\n",
        "                    chunks.append({\n",
        "                        \"text\": list_text,\n",
        "                        \"metadata\": {\n",
        "                            **metadata,\n",
        "                            \"content_type\": \"list\",\n",
        "                            \"keywords\": keywords,\n",
        "                            \"semantic_tags\": semantic_tags,\n",
        "                            \"embedded_links\": all_links\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "        # Process specific content types\n",
        "        if 'courses' in content and content['courses']:\n",
        "            for course in content['courses']:\n",
        "                course_text_parts = []\n",
        "                all_links = []\n",
        "                title_text = course.get('title', '')\n",
        "                course_text_parts.append(f\"Course: {title_text}\")\n",
        "\n",
        "                # Pass base_url to the helper function\n",
        "                desc_text, desc_links = self._extract_text_and_links(course.get('description', ''), base_url)\n",
        "                if desc_text:\n",
        "                    course_text_parts.append(f\"Description: {desc_text}\")\n",
        "                    all_links.extend(desc_links)\n",
        "\n",
        "                if 'details' in course and course['details']:\n",
        "                    detail_text_parts = []\n",
        "                    for item_html in course['details']:\n",
        "                        # Pass base_url to the helper function\n",
        "                        text, links = self._extract_text_and_links(item_html, base_url)\n",
        "                        detail_text_parts.append(text)\n",
        "                        all_links.extend(links)\n",
        "                    course_text_parts.append(\"Details:\\n• \" + \"\\n• \".join(detail_text_parts))\n",
        "\n",
        "                course_text = \"\\n\".join(course_text_parts)\n",
        "                keywords = self.extract_keywords(course_text)\n",
        "                semantic_tags = self.extract_semantic_tags(course_text)\n",
        "                keywords.append(\"course\")\n",
        "\n",
        "                chunks.append({\n",
        "                    \"text\": course_text,\n",
        "                    \"metadata\": {\n",
        "                        **metadata,\n",
        "                        \"content_type\": \"course\",\n",
        "                        \"keywords\": keywords,\n",
        "                        \"semantic_tags\": semantic_tags,\n",
        "                        \"course_title\": course.get('title', ''),\n",
        "                        \"embedded_links\": all_links\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # Process requirements if present\n",
        "        if 'requirements' in content and content['requirements']:\n",
        "            for req_title, req_content_html in content['requirements'].items():\n",
        "                req_text_parts = [f\"{req_title}:\"]\n",
        "                all_links = []\n",
        "\n",
        "                if isinstance(req_content_html, list):\n",
        "                    for item_html in req_content_html:\n",
        "                        # Pass base_url to the helper function\n",
        "                        text, links = self._extract_text_and_links(item_html, base_url)\n",
        "                        req_text_parts.append(text)\n",
        "                        all_links.extend(links)\n",
        "                else:\n",
        "                    # Pass base_url to the helper function\n",
        "                    text, links = self._extract_text_and_links(req_content_html, base_url)\n",
        "                    req_text_parts.append(text)\n",
        "                    all_links.extend(links)\n",
        "\n",
        "                req_text = \"\\n\".join(req_text_parts)\n",
        "                keywords = self.extract_keywords(req_text)\n",
        "                semantic_tags = self.extract_semantic_tags(req_text)\n",
        "                keywords.extend([\"requirement\", \"admission\"])\n",
        "\n",
        "                chunks.append({\n",
        "                    \"text\": req_text,\n",
        "                    \"metadata\": {\n",
        "                        **metadata,\n",
        "                        \"content_type\": \"requirement\",\n",
        "                        \"keywords\": keywords,\n",
        "                        \"semantic_tags\": semantic_tags,\n",
        "                        \"requirement_type\": req_title,\n",
        "                        \"embedded_links\": all_links\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # Process faculty information\n",
        "        if 'faculty' in content and content['faculty']:\n",
        "            for faculty in content['faculty']:\n",
        "                faculty_text_parts = [f\"Name: {faculty.get('name', '')}\"]\n",
        "                all_links = []\n",
        "\n",
        "                if 'title' in faculty:\n",
        "                    faculty_text_parts.append(f\"Title: {faculty['title']}\")\n",
        "\n",
        "                if 'bio' in faculty and faculty['bio']:\n",
        "                    bio_text_parts = []\n",
        "                    for item_html in faculty['bio']:\n",
        "                        # Pass base_url to the helper function\n",
        "                        text, links = self._extract_text_and_links(item_html, base_url)\n",
        "                        bio_text_parts.append(text)\n",
        "                        all_links.extend(links)\n",
        "                    faculty_text_parts.append(\"Bio: \" + \" \".join(bio_text_parts))\n",
        "\n",
        "                faculty_text = \"\\n\".join(faculty_text_parts)\n",
        "                keywords = self.extract_keywords(faculty_text)\n",
        "                semantic_tags = self.extract_semantic_tags(faculty_text)\n",
        "                keywords.extend([\"faculty\", \"professor\", \"instructor\"])\n",
        "\n",
        "                chunks.append({\n",
        "                    \"text\": faculty_text,\n",
        "                    \"metadata\": {\n",
        "                        **metadata,\n",
        "                        \"content_type\": \"faculty\",\n",
        "                        \"keywords\": keywords,\n",
        "                        \"semantic_tags\": semantic_tags,\n",
        "                        \"faculty_name\": faculty.get('name', ''),\n",
        "                        \"embedded_links\": all_links\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # Process tuition info if present\n",
        "        if 'tuition_info' in content and content['tuition_info']:\n",
        "            tuition_info = content['tuition_info']\n",
        "            tuition_text_parts = []\n",
        "            all_links = []\n",
        "\n",
        "            if 'tuition_text' in tuition_info:\n",
        "                for item_html in tuition_info['tuition_text']:\n",
        "                    # Pass base_url to the helper function\n",
        "                    text, links = self._extract_text_and_links(item_html, base_url)\n",
        "                    tuition_text_parts.append(text)\n",
        "                    all_links.extend(links)\n",
        "\n",
        "            if 'tuition_table' in tuition_info:\n",
        "                tuition_text_parts.append(\"\\nTuition Table:\")\n",
        "                for row in tuition_info['tuition_table']:\n",
        "                    row_text_parts = []\n",
        "                    for cell_html in row:\n",
        "                        # Pass base_url to the helper function\n",
        "                        text, links = self._extract_text_and_links(cell_html, base_url)\n",
        "                        row_text_parts.append(text)\n",
        "                        all_links.extend(links)\n",
        "                    tuition_text_parts.append(\" | \".join(row_text_parts))\n",
        "\n",
        "            tuition_text = \"\\n\".join(tuition_text_parts)\n",
        "\n",
        "            if tuition_text:\n",
        "                keywords = self.extract_keywords(tuition_text)\n",
        "                semantic_tags = self.extract_semantic_tags(tuition_text)\n",
        "                keywords.extend([\"tuition\", \"cost\", \"fee\", \"financial\"])\n",
        "\n",
        "                chunks.append({\n",
        "                    \"text\": tuition_text,\n",
        "                    \"metadata\": {\n",
        "                        **metadata,\n",
        "                        \"content_type\": \"tuition\",\n",
        "                        \"keywords\": keywords,\n",
        "                        \"semantic_tags\": semantic_tags,\n",
        "                        \"embedded_links\": all_links\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        # If no chunks were created, create one with headings\n",
        "        if not chunks and 'headings' in content and content['headings']:\n",
        "            heading_text = \"\\n\".join(content['headings'])\n",
        "            keywords = self.extract_keywords(heading_text)\n",
        "            semantic_tags = self.extract_semantic_tags(heading_text)\n",
        "\n",
        "            chunks.append({\n",
        "                \"text\": heading_text,\n",
        "                \"metadata\": {\n",
        "                    **metadata,\n",
        "                    \"content_type\": \"headings\",\n",
        "                    \"keywords\": keywords,\n",
        "                    \"semantic_tags\": semantic_tags,\n",
        "                    \"embedded_links\": []\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:\n",
        "        \"\"\"Extract important keywords from text using NLP techniques\"\"\"\n",
        "        doc = nlp(text)\n",
        "        potential_keywords = []\n",
        "        for ent in doc.ents:\n",
        "            potential_keywords.append(ent.text.lower())\n",
        "        for chunk in doc.noun_chunks:\n",
        "            potential_keywords.append(chunk.text.lower())\n",
        "        for token in doc:\n",
        "            if (token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADJ'] and\n",
        "                not token.is_stop and\n",
        "                len(token.text) > 2):\n",
        "                potential_keywords.append(token.lemma_.lower())\n",
        "        keyword_freq = {}\n",
        "        for kw in potential_keywords:\n",
        "            if len(kw) <= 2 or kw in self.stop_words:\n",
        "                continue\n",
        "            kw = re.sub(r'[^\\w\\s]', '', kw).strip()\n",
        "            if not kw:\n",
        "                continue\n",
        "            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1\n",
        "        for domain, terms in self.domain_keywords.items():\n",
        "            for term in terms:\n",
        "                if term in keyword_freq:\n",
        "                    keyword_freq[term] *= 1.5\n",
        "        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords = [k for k, v in sorted_keywords[:max_keywords]]\n",
        "        return list(set(top_keywords))\n",
        "\n",
        "    def extract_semantic_tags(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract semantic tags including dates, numbers, and special information types\"\"\"\n",
        "        semantic_tags = {}\n",
        "        doc = nlp(text)\n",
        "        dates = []\n",
        "        date_patterns = [\n",
        "            r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?,\\s+\\d{4}\\b',\n",
        "            r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n",
        "            r'\\b\\d{4}-\\d{1,2}-\\d{1,2}\\b',\n",
        "            r'\\b\\d{1,2}-\\d{1,2}-\\d{4}\\b',\n",
        "            r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}\\b',\n",
        "            r'\\bdeadline\\b.*?\\b\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}\\b',\n",
        "            r'\\bdue\\s+by\\b.*?\\b\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}\\b',\n",
        "            r'\\bdue\\s+date\\b.*?\\b\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}\\b'\n",
        "        ]\n",
        "        for pattern in date_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                dates.append(match.group(0))\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"DATE\":\n",
        "                dates.append(ent.text)\n",
        "        if dates:\n",
        "            semantic_tags[\"dates\"] = list(set(dates))\n",
        "        numbers = []\n",
        "        number_patterns = [\n",
        "            r'\\$\\d+(?:,\\d+)*(?:\\.\\d+)?',\n",
        "            r'\\d+(?:,\\d+)*\\s+dollars',\n",
        "            r'\\d+(?:\\.\\d+)?%',\n",
        "            r'\\d+(?:,\\d+)*(?:\\.\\d+)?\\s+credit(?:s)?',\n",
        "            r'\\d+(?:,\\d+)*(?:\\.\\d+)?\\s+hour(?:s)?',\n",
        "        ]\n",
        "        for pattern in number_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                numbers.append(match.group(0))\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"MONEY\", \"PERCENT\", \"QUANTITY\", \"CARDINAL\"]:\n",
        "                numbers.append(ent.text)\n",
        "        if numbers:\n",
        "            semantic_tags[\"numbers\"] = list(set(numbers))\n",
        "        info_type_patterns = {\n",
        "            \"application_info\": [r'application', r'apply', r'admit', r'admission', 'deadline', r'requirement'],\n",
        "            \"contact_info\": [r'contact', r'email', r'phone', r'@\\w+\\.\\w+', r'\\(\\d{3}\\)\\s*\\d{3}-\\d{4}', r'\\d{3}-\\d{3}-\\d{4}'],\n",
        "            \"course_info\": [r'course', r'class', r'curriculum', r'syllabus', r'credit'],\n",
        "            \"financial_info\": [r'tuition', r'cost', 'fee', r'financial aid', r'scholarship', r'grant', r'loan'],\n",
        "            \"faculty_info\": [r'faculty', r'professor', r'instructor', r'teacher',r'staff'],\n",
        "            \"career_info\": [r'career', r'job', 'employment', r'placement', r'opportunity', r'alumni']\n",
        "        }\n",
        "        for info_type, patterns in info_type_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, text, re.IGNORECASE):\n",
        "                    semantic_tags[info_type] = True\n",
        "                    break\n",
        "        urls = re.findall(r'https?://\\S+', text)\n",
        "        if urls:\n",
        "            semantic_tags[\"contains_urls\"] = urls\n",
        "        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "        if emails:\n",
        "            semantic_tags[\"contains_emails\"] = emails\n",
        "        return semantic_tags\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def save_processed_data(self, output_file: str):\n",
        "        \"\"\"Save processed chunks to a JSON file\"\"\"\n",
        "        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.processed_chunks, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Processed data saved to {output_file}\")\n",
        "        if not self.processed_chunks:\n",
        "            print(\"No chunks were processed, cannot generate statistics.\")\n",
        "            return\n",
        "        keywords_per_chunk = [len(chunk['metadata'].get('keywords', [])) for chunk in self.processed_chunks]\n",
        "        semantic_tags_per_chunk = [len(chunk['metadata'].get('semantic_tags', {})) for chunk in self.processed_chunks]\n",
        "        chunks_with_links = sum(1 for chunk in self.processed_chunks if chunk[\"metadata\"].get(\"embedded_links\"))\n",
        "        print(f\"Average keywords per chunk: {sum(keywords_per_chunk)/len(keywords_per_chunk):.2f}\")\n",
        "        print(f\"Average semantic tags per chunk: {sum(semantic_tags_per_chunk)/len(semantic_tags_per_chunk):.2f}\")\n",
        "        print(f\"Chunks containing embedded links: {chunks_with_links} (out of {len(self.processed_chunks)})\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    script_dir = os.path.dirname(__file__) if '__file__' in locals() else os.getcwd()\n",
        "    input_data_path = os.path.join(script_dir, \"data/ms_applied_data_science_data.json\")\n",
        "    output_data_path = os.path.join(script_dir, \"data/ms_applied_data_science_enhanced_chunks.json\")\n",
        "\n",
        "    if not os.path.exists(input_data_path):\n",
        "        print(f\"Error: Input file not found at {input_data_path}\")\n",
        "        print(\"Please make sure 'ms_applied_data_science_data.json' is in a 'data' folder.\")\n",
        "    else:\n",
        "        preprocessor = EnhancedDataPreprocessor(input_data_path)\n",
        "        chunks = preprocessor.preprocess()\n",
        "\n",
        "        # --- THIS IS THE LINE I FIXED (I added the parentheses) ---\n",
        "        if chunks:\n",
        "            preprocessor.save_processed_data(output_data_path)\n",
        "        else:\n",
        "            print(\"Preprocessing finished, but no chunks were created. Output file not saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
