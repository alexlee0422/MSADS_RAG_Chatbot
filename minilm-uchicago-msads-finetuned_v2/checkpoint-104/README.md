---
language:
- en
license: apache-2.0
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:829
- loss:MatryoshkaLoss
- loss:MultipleNegativesRankingLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: Can students with significant full-time work experience waive the
    Career Seminar course?
  sentences:
  - '‚Ä¢ Career Seminar (Required) Pass/Fail The Career Seminar (Pass/Fail) supports
    the development of industry professional skills, job and/or internship searches,
    and other in-demand areas of competency among today‚Äôs employers. Students enroll
    in the Career Seminar each quarter in order to engage in unique content throughout
    their degree program. Students with significant full-time work experience may
    be eligible to waive this course. 0 units, no cost.

    ‚Ä¢ Machine Learning II Letter Grade The objective of this course is three-folds‚Äìfirst,
    to extend student understanding of predictive modeling with machine learning concepts
    and methodologies from Machine Learning 1 into the realm of Deep Learning and
    Generative AI. Second, to develop the ability to apply those concepts and methodologies
    to diverse practical applications, evaluate the results and recommend the next
    best action. Third, to discuss and understand state-of-the machine learning and
    deep learning research and development and their applications.

    ‚Ä¢ Elective 2 Letter Grade Elective offerings vary. Students will work with their
    academic advisor to select electives based on their interests and course availability.
    Past electives include: Generative AI Principles, Advanced Computer Vision with
    Deep Learning, Advanced Machine Learning and Artificial Intelligence, Bayesian
    Machine Learning with GenAI Applications, Data Science for Algorithmic Marketing,
    Data Visualization Techniques, Digital Marketing Analytics in Theory and Practice,
    Financial Analytics, Health Analytics, Machine Learning Operations, Natural Language
    Processing and Cognitive Computing, Real Time Intelligent Systems, Reinforcement
    Learning, Supply Chain Optimization.

    ‚Ä¢ Data Science Capstone Project Letter Grade The required Capstone Project is
    completed over two quarters and covers research design, implementation, and writing.
    Full-time students start their capstone project in their third quarter. Part-time
    students generally begin the capstone project in their fifth quarter.

    ‚Ä¢ Career Seminar (Required) Pass/Fail The Career Seminar (Pass/Fail) supports
    the development of industry professional skills, job and/or internship searches,
    and other in-demand areas of competency among today‚Äôs employers. Students enroll
    in the Career Seminar each quarter in order to engage in unique content throughout
    their degree program. Students with significant full-time work experience may
    be eligible to waive this course. 0 units, no cost.'
  - The Pass/Fail Career Seminar supports the development of industry professional
    skills, job and/or internship searches, and other in-demand areas of competency
    among today‚Äôs employers. Students enroll in the Career Seminar each quarter in
    order to engage in unique content throughout their degree program. Students with
    significant full-time work experience may be eligible to waive this course. 0
    units, no cost.
  - 'Course: Unknown Course

    Description: Elective offerings vary. Students will work with their academic advisor
    to select electives based on their interests and course availability. Past electives
    include: Generative AI Principles, Advanced Computer Vision with Deep Learning,
    Advanced Machine Learning and Artificial Intelligence, Bayesian Machine Learning
    with GenAI Applications, Data Science for Algorithmic Marketing, Data Visualization
    Techniques, Digital Marketing Analytics in Theory and Practice, Financial Analytics,
    Health Analytics, Machine Learning Operations, Natural Language Processing and
    Cognitive Computing, Real Time Intelligent Systems, Reinforcement Learning, Supply
    Chain Optimization.

    Details:

    ‚Ä¢ Elective 4 Letter Grade Elective offerings vary. Students will work with their
    academic advisor to select electives based on their interests and course availability.
    Past electives include: Generative AI Principles, Advanced Computer Vision with
    Deep Learning, Advanced Machine Learning and Artificial Intelligence, Bayesian
    Machine Learning with GenAI Applications, Data Science for Algorithmic Marketing,
    Data Visualization Techniques, Digital Marketing Analytics in Theory and Practice,
    Financial Analytics, Health Analytics, Machine Learning Operations, Natural Language
    Processing and Cognitive Computing, Real Time Intelligent Systems, Reinforcement
    Learning, Supply Chain Optimization.'
- source_sentence: What elective courses are available in the University of Chicago's
    MS in Applied Data Science program for students who choose the longer academic
    experience option?
  sentences:
  - Greg‚Äôs industry roles include Chief Analytics Officer at Harland Clarke Holdings,
    Director at Google, EVP/Managing Director at Publicis Groupe, and Analytics Practice
    Lead at PwC. Greg‚Äôs patented cloud-based media analytics platform was highlighted
    in Harvard Business Review and Fast Company .
  - This course dives into the realm of Generative AI, offering a comprehensive look
    into the world of Large Language Models (LLMs), image generation techniques, and
    the fusion of vision and text through multimodal models. Drawing from core concepts
    in neural networks, transformers, and advanced techniques such as prompt engineering,
    vision prompting, and multimodality representation, students will explore the
    capabilities, applications, and ethical considerations of generative models. This
    course culminates in hands-on projects, allowing participants to apply theory
    to practical scenarios.
  - This option is ideal for students seeking a longer academic experience with more
    time to complete more elective courses. Students must also have the capacity to
    complete a traditional written thesis or thesis project.
- source_sentence: What opportunities does the University of Chicago's MS in Applied
    Data Science program offer for students to learn about prompt engineering and
    its potential in guiding machine learning models?
  sentences:
  - As an In-Person program student, you will have access to expert faculty and instructors
    with industry expertise, a full-service student affairs team, and an unparalleled
    network of global alumni . Our team is passionate about supporting a Signature
    Student Experience tailored to your needs.
  - '

    ‚Ä¢ Overview

    ‚Ä¢ In-Person Program

    ‚Ä¢ Online Program

    ‚Ä¢ Capstone Projects

    ‚Ä¢ Course Progressions

    ‚Ä¢ How to Apply

    ‚Ä¢ Events & Deadlines

    ‚Ä¢ Tuition, Fees, & Aid

    ‚Ä¢ Our Students

    ‚Ä¢ Faculty, Instructors, Staff

    ‚Ä¢ FAQs

    ‚Ä¢ Explore the MS-ADS Campus

    ‚Ä¢ Career Outcomes

    ‚Ä¢ Get In Touch'
  - What excites me right now about the field of data science is the concept of prompt
    engineering and its potential. By carefully designing prompts, we can guide machine
    learning models to generate desired outputs, enabling us to tackle complex problems
    and extract valuable insights. This innovative approach not only enhances the
    interpretability and control of machine learning systems but also opens up new
    possibilities for driving impactful advancements in data science.
- source_sentence: What specific software packages will be covered in the Data Engineering
    Platforms and Big Data and Cloud Computing courses?
  sentences:
  - UChicago‚Äôs Master‚Äôs in Applied Data Science program recruits industry leaders
    from across the country. Meet our instructors and learn how they are shaping the
    landscape of data science.
  - With over a decade of experience in higher education, Taylor brings a strong background
    in international comparative education and foreign credential evaluation. Her
    interest in data science was sparked during her graduate studies, where she discovered
    a passion for integrating data-driven approaches into education.
  - 'Course: Unknown Course

    Description: Data Engineering Platforms teaches effective data engineering‚Äîan
    essential first step in building an analytics-driven competitive advantage in
    the market.¬†Big Data and Cloud Computing teaches students how to approach big
    data and large-scale machine learning applications. There is no single definition
    of big data and multiple emerging software packages exist to work with it, and
    we will cover the most popular approaches.

    Details:

    ‚Ä¢ Data Engineering Platforms for Analytics or Big Data and Cloud Computing Letter
    Grade Data Engineering Platforms teaches effective data engineering‚Äîan essential
    first step in building an analytics-driven competitive advantage in the market.¬†Big
    Data and Cloud Computing teaches students how to approach big data and large-scale
    machine learning applications. There is no single definition of big data and multiple
    emerging software packages exist to work with it, and we will cover the most popular
    approaches.'
- source_sentence: What specific expertise does John Navarro bring to the Master of
    Science in Applied Data Science program at the University of Chicago?
  sentences:
  - This Foundational, optional course is held in the 5 weeks leading up to the start
    of your first quarter. This course is an introduction to the essential concepts
    and techniques for the statistical computing language R. 0 units, no cost.
  - 'Extracting actionable insights from unstructured text and designing cognitive
    applications have become significant areas of application for analytics. Students
    in this course will learn foundations of natural language processing, including:
    concept extraction; text summarization and topic modeling; part of speech tagging;
    named entity recognition; semantic roles and sentiment analysis. For advanced
    NLP applications, we will focus on feature extraction from unstructured text,
    including word and paragraph embedding and representing words and paragraphs as
    vectors. For cognitive analytics section of the course, students will practice
    designing question answering systems with intent classification, semantic knowledge
    extraction and reasoning under uncertainty. Students will gain hands-on expertise
    applying Python for text analysis tasks, as well as practice with multiple IBM
    Watson services, including: Watson Discovery, Watson Conversation, Watson Natural
    Language Classification and Watson Natural Language Understanding.'
  - John Navarro is an Adjunct Instructor in the Master of Science in Applied Data
    Science program at the University of Chicago, bringing a unique blend of academic
    expertise and real-world industry experience to the classroom. He currently serves
    as Chief Financial Officer at John Burns Construction Company, where he leads
    financial operations, drives long-term strategic planning, and implements advanced
    analytics to guide organizational decision-making.
pipeline_tag: sentence-similarity
library_name: sentence-transformers
metrics:
- cosine_accuracy@1
- cosine_accuracy@3
- cosine_accuracy@5
- cosine_accuracy@10
- cosine_precision@1
- cosine_precision@3
- cosine_precision@5
- cosine_precision@10
- cosine_recall@1
- cosine_recall@3
- cosine_recall@5
- cosine_recall@10
- cosine_ndcg@10
- cosine_mrr@10
- cosine_map@100
model-index:
- name: all-MiniLM-L6-v2-uchicago-msads-matryoshka_v2
  results:
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: dim 384
      type: dim_384
    metrics:
    - type: cosine_accuracy@1
      value: 0.2903225806451613
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.45161290322580644
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.5053763440860215
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.6236559139784946
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.2903225806451613
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.15053763440860216
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.10107526881720429
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.06236559139784945
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.2903225806451613
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.45161290322580644
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.5053763440860215
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.6236559139784946
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.444042589782387
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.3882445809865165
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.40618431288983675
      name: Cosine Map@100
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: dim 256
      type: dim_256
    metrics:
    - type: cosine_accuracy@1
      value: 0.26881720430107525
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.43010752688172044
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.5161290322580645
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.6344086021505376
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.26881720430107525
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.14336917562724014
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.10322580645161288
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.06344086021505375
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.26881720430107525
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.43010752688172044
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.5161290322580645
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.6344086021505376
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.4356533780489022
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.37362177846048816
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.3907615839515957
      name: Cosine Map@100
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: dim 128
      type: dim_128
    metrics:
    - type: cosine_accuracy@1
      value: 0.25806451612903225
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.44086021505376344
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.5376344086021505
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.6344086021505376
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.25806451612903225
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.14695340501792115
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.1075268817204301
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.06344086021505375
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.25806451612903225
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.44086021505376344
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.5376344086021505
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.6344086021505376
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.4357763298492021
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.37308841099163687
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.38873363388623194
      name: Cosine Map@100
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: dim 64
      type: dim_64
    metrics:
    - type: cosine_accuracy@1
      value: 0.25806451612903225
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.3763440860215054
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.4838709677419355
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.6129032258064516
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.25806451612903225
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.12544802867383514
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.09677419354838705
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.061290322580645144
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.25806451612903225
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.3763440860215054
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.4838709677419355
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.6129032258064516
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.4115135070533144
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.3494410308926438
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.3639919960809236
      name: Cosine Map@100
---

# all-MiniLM-L6-v2-uchicago-msads-matryoshka_v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision c9745ed1d9f207416be6d2e6f8de32d1f16199bf -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
- **Language:** en
- **License:** apache-2.0

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'What specific expertise does John Navarro bring to the Master of Science in Applied Data Science program at the University of Chicago?',
    'John Navarro is an Adjunct Instructor in the Master of Science in Applied Data Science program at the University of Chicago, bringing a unique blend of academic expertise and real-world industry experience to the classroom. He currently serves as Chief Financial Officer at John Burns Construction Company, where he leads financial operations, drives long-term strategic planning, and implements advanced analytics to guide organizational decision-making.',
    'Extracting actionable insights from unstructured text and designing cognitive applications have become significant areas of application for analytics. Students in this course will learn foundations of natural language processing, including: concept extraction; text summarization and topic modeling; part of speech tagging; named entity recognition; semantic roles and sentiment analysis. For advanced NLP applications, we will focus on feature extraction from unstructured text, including word and paragraph embedding and representing words and paragraphs as vectors. For cognitive analytics section of the course, students will practice designing question answering systems with intent classification, semantic knowledge extraction and reasoning under uncertainty. Students will gain hands-on expertise applying Python for text analysis tasks, as well as practice with multiple IBM Watson services, including: Watson Discovery, Watson Conversation, Watson Natural Language Classification and Watson Natural Language Understanding.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.7261, 0.1476],
#         [0.7261, 1.0000, 0.2093],
#         [0.1476, 0.2093, 1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

## Evaluation

### Metrics

#### Information Retrieval

* Dataset: `dim_384`
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator) with these parameters:
  ```json
  {
      "truncate_dim": 384
  }
  ```

| Metric              | Value     |
|:--------------------|:----------|
| cosine_accuracy@1   | 0.2903    |
| cosine_accuracy@3   | 0.4516    |
| cosine_accuracy@5   | 0.5054    |
| cosine_accuracy@10  | 0.6237    |
| cosine_precision@1  | 0.2903    |
| cosine_precision@3  | 0.1505    |
| cosine_precision@5  | 0.1011    |
| cosine_precision@10 | 0.0624    |
| cosine_recall@1     | 0.2903    |
| cosine_recall@3     | 0.4516    |
| cosine_recall@5     | 0.5054    |
| cosine_recall@10    | 0.6237    |
| **cosine_ndcg@10**  | **0.444** |
| cosine_mrr@10       | 0.3882    |
| cosine_map@100      | 0.4062    |

#### Information Retrieval

* Dataset: `dim_256`
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator) with these parameters:
  ```json
  {
      "truncate_dim": 256
  }
  ```

| Metric              | Value      |
|:--------------------|:-----------|
| cosine_accuracy@1   | 0.2688     |
| cosine_accuracy@3   | 0.4301     |
| cosine_accuracy@5   | 0.5161     |
| cosine_accuracy@10  | 0.6344     |
| cosine_precision@1  | 0.2688     |
| cosine_precision@3  | 0.1434     |
| cosine_precision@5  | 0.1032     |
| cosine_precision@10 | 0.0634     |
| cosine_recall@1     | 0.2688     |
| cosine_recall@3     | 0.4301     |
| cosine_recall@5     | 0.5161     |
| cosine_recall@10    | 0.6344     |
| **cosine_ndcg@10**  | **0.4357** |
| cosine_mrr@10       | 0.3736     |
| cosine_map@100      | 0.3908     |

#### Information Retrieval

* Dataset: `dim_128`
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator) with these parameters:
  ```json
  {
      "truncate_dim": 128
  }
  ```

| Metric              | Value      |
|:--------------------|:-----------|
| cosine_accuracy@1   | 0.2581     |
| cosine_accuracy@3   | 0.4409     |
| cosine_accuracy@5   | 0.5376     |
| cosine_accuracy@10  | 0.6344     |
| cosine_precision@1  | 0.2581     |
| cosine_precision@3  | 0.147      |
| cosine_precision@5  | 0.1075     |
| cosine_precision@10 | 0.0634     |
| cosine_recall@1     | 0.2581     |
| cosine_recall@3     | 0.4409     |
| cosine_recall@5     | 0.5376     |
| cosine_recall@10    | 0.6344     |
| **cosine_ndcg@10**  | **0.4358** |
| cosine_mrr@10       | 0.3731     |
| cosine_map@100      | 0.3887     |

#### Information Retrieval

* Dataset: `dim_64`
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator) with these parameters:
  ```json
  {
      "truncate_dim": 64
  }
  ```

| Metric              | Value      |
|:--------------------|:-----------|
| cosine_accuracy@1   | 0.2581     |
| cosine_accuracy@3   | 0.3763     |
| cosine_accuracy@5   | 0.4839     |
| cosine_accuracy@10  | 0.6129     |
| cosine_precision@1  | 0.2581     |
| cosine_precision@3  | 0.1254     |
| cosine_precision@5  | 0.0968     |
| cosine_precision@10 | 0.0613     |
| cosine_recall@1     | 0.2581     |
| cosine_recall@3     | 0.3763     |
| cosine_recall@5     | 0.4839     |
| cosine_recall@10    | 0.6129     |
| **cosine_ndcg@10**  | **0.4115** |
| cosine_mrr@10       | 0.3494     |
| cosine_map@100      | 0.364      |

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 829 training samples
* Columns: <code>anchor</code> and <code>positive</code>
* Approximate statistics based on the first 829 samples:
  |         | anchor                                                                             | positive                                                                            |
  |:--------|:-----------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|
  | type    | string                                                                             | string                                                                              |
  | details | <ul><li>min: 15 tokens</li><li>mean: 28.05 tokens</li><li>max: 60 tokens</li></ul> | <ul><li>min: 6 tokens</li><li>mean: 114.45 tokens</li><li>max: 256 tokens</li></ul> |
* Samples:
  | anchor                                                                                                                                                                                                                                            | positive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
  |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>What are the specific admission requirements for the MS in Applied Data Science program at the University of Chicago?</code>                                                                                                                | <code>Course: Unknown Course</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
  | <code>What statistical models are covered in the University of Chicago's MS in Applied Data Science program, specifically in relation to the "Linear Normal" assumption commonly used in statistics/data science for almost two centuries?</code> | <code>Course: Unknown Course<br>Description: In a traditional linear model, the observed response follows a normal distribution, and the expected response value is a linear combination of the predictors.¬† Since Carl Friedrich Gauss (1777-1855) and Adrien-Marie Legendre (1752-1833) created this linear model framework in the early 1800s, the ‚ÄúLinear Normal‚Äù assumption has been the norm in statistics/data science for almost two centuries.¬† New methods based on probability distributions other than Gaussian appeared only in the second half of the twentieth century. These methods allowed working with variables that span a broader variety of domains and probability distributions. Besides, methods for the analysis of general associations were developed that are different from the Pearson correlation.<br>Details:<br>‚Ä¢ Statistical Models for Data Science Letter Grade In a traditional linear model, the observed response follows a normal distribution, and the expected response value is a linear combination of th...</code> |
  | <code>Can you provide more information on the software packages used in the Big Data and Cloud Computing course at the University of Chicago's MS in Applied Data Science program?</code>                                                         | <code>Big Data and Cloud Computing teaches students how to approach big data and large-scale machine learning applications. There is no single definition of big data and multiple emerging software packages exist to work with it, and we will cover the most popular approaches.</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
* Loss: [<code>MatryoshkaLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#matryoshkaloss) with these parameters:
  ```json
  {
      "loss": "MultipleNegativesRankingLoss",
      "matryoshka_dims": [
          384,
          256,
          128,
          64
      ],
      "matryoshka_weights": [
          1,
          1,
          1,
          1
      ],
      "n_dims_per_step": -1
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `eval_strategy`: epoch
- `per_device_train_batch_size`: 4
- `per_device_eval_batch_size`: 2
- `gradient_accumulation_steps`: 8
- `learning_rate`: 2e-05
- `num_train_epochs`: 5
- `warmup_ratio`: 0.1
- `load_best_model_at_end`: True
- `optim`: adamw_torch
- `batch_sampler`: no_duplicates

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: epoch
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 4
- `per_device_eval_batch_size`: 2
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 8
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 2e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1.0
- `num_train_epochs`: 5
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.1
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: True
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `parallelism_config`: None
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `project`: huggingface
- `trackio_space_id`: trackio
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `hub_revision`: None
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: no
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `liger_kernel_config`: None
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: True
- `prompts`: None
- `batch_sampler`: no_duplicates
- `multi_dataset_batch_sampler`: proportional
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Training Logs
| Epoch  | Step | Training Loss | dim_384_cosine_ndcg@10 | dim_256_cosine_ndcg@10 | dim_128_cosine_ndcg@10 | dim_64_cosine_ndcg@10 |
|:------:|:----:|:-------------:|:----------------------:|:----------------------:|:----------------------:|:---------------------:|
| 0.9615 | 25   | 1.8774        | -                      | -                      | -                      | -                     |
| 1.0    | 26   | -             | 0.4149                 | 0.4031                 | 0.4190                 | 0.3721                |
| 1.9231 | 50   | 0.5887        | -                      | -                      | -                      | -                     |
| 2.0    | 52   | -             | 0.4086                 | 0.3967                 | 0.3953                 | 0.3914                |
| 2.8846 | 75   | 0.4201        | -                      | -                      | -                      | -                     |
| 3.0    | 78   | -             | 0.4303                 | 0.4186                 | 0.4256                 | 0.4247                |
| 3.8462 | 100  | 0.3929        | -                      | -                      | -                      | -                     |
| 4.0    | 104  | -             | 0.4440                 | 0.4357                 | 0.4358                 | 0.4115                |


### Framework Versions
- Python: 3.12.1
- Sentence Transformers: 5.1.1
- Transformers: 4.57.1
- PyTorch: 2.9.0
- Accelerate: 1.6.0
- Datasets: 4.2.0
- Tokenizers: 0.22.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MatryoshkaLoss
```bibtex
@misc{kusupati2024matryoshka,
    title={Matryoshka Representation Learning},
    author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},
    year={2024},
    eprint={2205.13147},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->