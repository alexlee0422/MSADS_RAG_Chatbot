{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Sentence-Transformers (for fine-tuning) Imports\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerModelCardData\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers import SentenceTransformerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely get the OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original file: ms_applied_data_science_enhanced_chunks.json...\n",
      "Processing 802 chunks...\n",
      "Splitting any chunk over 3000 characters.\n",
      "\n",
      "Saving new fixed-size-split file to: ms_applied_data_science_RE_CHUNKED_FIXED_SIZE.json...\n",
      "\n",
      "--- Fixed-Size Re-chunking Complete ---\n",
      "Original chunks processed: 802\n",
      "Chunks kept (under threshold): 788\n",
      "Large chunks split: 14\n",
      "New sub-chunks created: 134\n",
      "Total chunks in new file: 922\n"
     ]
    }
   ],
   "source": [
    "# --- NEW CELL (Call it 'Cell 3.5') ---\n",
    "# SCRIPT TO SPLIT LARGE CHUNKS BY FIXED SIZE (NO DE-DUPLICATION)\n",
    "\n",
    "import json\n",
    "# We'll use LangChain's text splitter for this\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "ORIGINAL_FILE = \"ms_applied_data_science_enhanced_chunks.json\"\n",
    "NEW_FILE = \"ms_applied_data_science_RE_CHUNKED_FIXED_SIZE.json\"\n",
    "\n",
    "# --- DYNAMIC SETTINGS ---\n",
    "# We will split any chunk larger than this.\n",
    "CHARACTER_THRESHOLD = 3000\n",
    "# We add a small overlap to maintain context between the new split chunks\n",
    "CHUNK_OVERLAP = 150\n",
    "# --- END SETTINGS ---\n",
    "\n",
    "# 1. Set up our text splitter\n",
    "# This will split text into chunks of 3000 chars\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHARACTER_THRESHOLD,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Loading original file: {ORIGINAL_FILE}...\")\n",
    "with open(ORIGINAL_FILE, 'r', encoding='utf-8') as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "new_chunk_list = []\n",
    "original_chunk_count = len(original_data)\n",
    "chunks_kept_as_is = 0\n",
    "chunks_split = 0\n",
    "new_sub_chunks_created = 0\n",
    "\n",
    "print(f\"Processing {original_chunk_count} chunks...\")\n",
    "print(f\"Splitting any chunk over {CHARACTER_THRESHOLD} characters.\")\n",
    "\n",
    "for i, chunk_item in enumerate(original_data):\n",
    "    text_to_process = chunk_item.get('text', '')\n",
    "    base_metadata = chunk_item.get('metadata', {})\n",
    "\n",
    "    # 1. Check if chunk is *smaller* than the threshold\n",
    "    if len(text_to_process) <= CHARACTER_THRESHOLD:\n",
    "        # It's a \"good\" chunk. Keep it as-is.\n",
    "        new_chunk_list.append(chunk_item)\n",
    "        chunks_kept_as_is += 1\n",
    "\n",
    "    # 2. If it's *larger*, we split it\n",
    "    else:\n",
    "        chunks_split += 1\n",
    "        # print(f\"--- Splitting large chunk (index {i}, length {len(text_to_process)}) ---\")\n",
    "\n",
    "        # Use the splitter to create new, smaller text chunks\n",
    "        sub_texts = text_splitter.split_text(text_to_process)\n",
    "\n",
    "        for sub_text in sub_texts:\n",
    "            # Create a new item for each new sub-chunk\n",
    "            new_item = {\n",
    "                \"text\": sub_text,\n",
    "                \"metadata\": base_metadata.copy() # Inherit metadata\n",
    "            }\n",
    "            new_chunk_list.append(new_item)\n",
    "            new_sub_chunks_created += 1\n",
    "\n",
    "# --- Save the new file ---\n",
    "print(f\"\\nSaving new fixed-size-split file to: {NEW_FILE}...\")\n",
    "with open(NEW_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_chunk_list, f, indent=2)\n",
    "\n",
    "print(\"\\n--- Fixed-Size Re-chunking Complete ---\")\n",
    "print(f\"Original chunks processed: {original_chunk_count}\")\n",
    "print(f\"Chunks kept (under threshold): {chunks_kept_as_is}\")\n",
    "print(f\"Large chunks split: {chunks_split}\")\n",
    "print(f\"New sub-chunks created: {new_sub_chunks_created}\")\n",
    "print(f\"Total chunks in new file: {len(new_chunk_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ms_applied_data_science_RE_CHUNKED_FIXED_SIZE.json...\n",
      "Successfully loaded and created 922 Document nodes.\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for your JSON data\n",
    "DATA_FILE = \"ms_applied_data_science_RE_CHUNKED_FIXED_SIZE.json\"\n",
    "\n",
    "# Load the JSON data\n",
    "print(f\"Loading data from {DATA_FILE}...\")\n",
    "try:\n",
    "    with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {DATA_FILE} not found.\")\n",
    "    print(\"Please make sure the file is in the same directory as this script.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from {DATA_FILE}.\")\n",
    "\n",
    "# Convert JSON data into LangChain Document objects (our \"nodes\")\n",
    "# We can skip PyPDFLoader and RecursiveCharacterTextSplitter because our JSON is already pre-chunked.\n",
    "nodes = []\n",
    "for i, item in enumerate(json_data):\n",
    "    # We create a Document object for each chunk.\n",
    "    doc = Document(\n",
    "        page_content=item.get('text', ''),\n",
    "        metadata=item.get('metadata', {})\n",
    "    )\n",
    "    doc.metadata[\"id\"] = f\"node_{i}\"\n",
    "    nodes.append(doc)\n",
    "\n",
    "print(f\"Successfully loaded and created {len(nodes)} Document nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Showing first 3 of 922 loaded nodes ---\n",
      "\n",
      "--- NODE 0 (Chunk 0) ---\n",
      "CONTENT:\n",
      "The University of Chicago’s MS in Applied Data Science program equips you with\n",
      "in-demand expertise and an unparalleled network of global alumni. Take the next\n",
      "step and start your application today.\n",
      "\n",
      "METADATA:\n",
      "{'source': 'https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/', 'title': \"Master's in Applied Data Science - DSI\", 'page_type': 'main', 'last_scraped': '2025-11-07 18:20:38', 'content_type': 'paragraph', 'keywords': ['the university of chicagos ms', 'applied data science program', 'an unparalleled network', 'global alumni', 'demand', 'program', 'chicago', 'application', 'today', 'applied data science'], 'semantic_tags': {'dates': ['today'], 'application_info': True, 'career_info': True}, 'embedded_links': [], 'id': 'node_0'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- NODE 1 (Chunk 1) ---\n",
      "CONTENT:\n",
      "Choose from full- and part-time options in our In-Person and Online programs.\n",
      "Apply today!\n",
      "\n",
      "METADATA:\n",
      "{'source': 'https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/', 'title': \"Master's in Applied Data Science - DSI\", 'page_type': 'main', 'last_scraped': '2025-11-07 18:20:38', 'content_type': 'paragraph', 'keywords': ['full', 'apply', 'time', 'program', 'today', 'choose', 'option', 'person', 'our inperson and online programs', 'full and parttime options'], 'semantic_tags': {'dates': ['today'], 'application_info': True}, 'embedded_links': [{'text': 'Apply today!', 'href': 'https://apply-psd.uchicago.edu/apply/'}], 'id': 'node_1'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- NODE 2 (Chunk 2) ---\n",
      "CONTENT:\n",
      "The application for entry is open! Prospective Online and In-Person students can\n",
      "apply by June 23, 2026, for full- and part-time study for Autumn 2026 entry.\n",
      "Start your application here .\n",
      "\n",
      "METADATA:\n",
      "{'source': 'https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/', 'title': \"Master's in Applied Data Science - DSI\", 'page_type': 'main', 'last_scraped': '2025-11-07 18:20:38', 'content_type': 'paragraph', 'keywords': ['autumn 2026', 'june 23 2026', 'apply', 'person', 'application', 'the application', 'student', 'study', 'june', 'entry'], 'semantic_tags': {'dates': ['June 23, 2026', 'Autumn 2026'], 'application_info': True}, 'embedded_links': [{'text': 'here', 'href': 'https://apply-psd.uchicago.edu/apply/'}], 'id': 'node_2'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# The 'nodes' variable was created in the cell above.\n",
    "# Let's inspect the first 3 nodes to see what they look like.\n",
    "\n",
    "print(f\"--- Showing first 3 of {len(nodes)} loaded nodes ---\")\n",
    "\n",
    "for i, node in enumerate(nodes[:3]):\n",
    "    print(f\"\\n--- NODE {i} (Chunk {i}) ---\")\n",
    "\n",
    "    # Wrap the text for cleaner printing\n",
    "    print(f\"CONTENT:\\n{textwrap.fill(node.page_content, 80)}\")\n",
    "\n",
    "    # Print the metadata\n",
    "    print(f\"\\nMETADATA:\\n{node.metadata}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data for Fine-Tuning Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_batch(nodes, llm, num_questions_per_chunk=1, batch_size=20):\n",
    "    \"\"\"\n",
    "    [NEW BATCH VERSION]\n",
    "    Generates questions in parallel batches to speed up the process.\n",
    "    \"\"\"\n",
    "    # Your new \"student\" prompt\n",
    "    prompt_template_str = \"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge.\n",
    "    generate only questions based on the below query.\n",
    "\n",
    "    You are a prospective student or current student interested in the \\\n",
    "    University of Chicago's MS in Applied Data Science program. \\\n",
    "    Your task is to ask {num_questions_per_chunk} specific questions about \\\n",
    "    the program details based *only* on the context provided. \\\n",
    "    The questions should be typical of someone asking a chatbot for details.\n",
    "\n",
    "    Restrict your questions *only* to the information in the context.\n",
    "    Do not ask questions if the context does not provide the answer.\"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    # 1. Prepare all inputs for the batch\n",
    "    inputs = []\n",
    "    for node in nodes:\n",
    "        inputs.append({\n",
    "            \"context_str\": node.page_content,\n",
    "            \"num_questions_per_chunk\": num_questions_per_chunk\n",
    "        })\n",
    "\n",
    "    print(f\"Generating questions for {len(inputs)} nodes in parallel (max concurrency: {batch_size})...\")\n",
    "\n",
    "    # 2. Run the batch processing (this will show a tqdm progress bar!)\n",
    "    # We set max_concurrency to avoid hitting rate limits too hard.\n",
    "    responses = chain.batch(inputs, config={\"max_concurrency\": batch_size})\n",
    "\n",
    "    # 3. Process the responses\n",
    "    print(\"Processing responses...\")\n",
    "    for i, response in enumerate(responses):\n",
    "        node = nodes[i]\n",
    "        node_id = node.metadata[\"id\"]\n",
    "\n",
    "        result = response.strip().split(\"\\n\")\n",
    "        questions = [re.sub(r\"^\\d+[\\\\).\\\\s]\", \"\", q).strip() for q in result]\n",
    "        questions = [q for q in questions if len(q) > 0]\n",
    "\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [node_id]\n",
    "\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing synthetic data from uchicago_synthetic_data_v2.pkl...\n",
      "Loaded 922 existing questions.\n",
      "Found 922 already processed nodes.\n",
      "All nodes have already been processed! Nothing to do.\n",
      "\n",
      "--- Process complete. Total questions in file: 922 ---\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS THE NEW CRASH-PROOF RESUMABLE EXECUTION CELL ---\n",
    "import time\n",
    "from openai import RateLimitError # Need to import the error to catch it\n",
    "\n",
    "synthetic_data_file = \"uchicago_synthetic_data_v2.pkl\"\n",
    "\n",
    "# --- BATCH SETTINGS ---\n",
    "# This is the \"super-batch\" for saving. We'll save our progress every 50 nodes.\n",
    "SUPER_BATCH_SIZE = 50\n",
    "# This is the \"inner-batch\" for API calls.\n",
    "# We set it low (e.g., 5) to prevent rate limit errors.\n",
    "INNER_BATCH_SIZE = 5\n",
    "# --- END SETTINGS ---\n",
    "\n",
    "# 1. Check for an existing progress file\n",
    "# (This logic is correct and remains the same)\n",
    "if os.path.exists(synthetic_data_file):\n",
    "    print(f\"Loading existing synthetic data from {synthetic_data_file}...\")\n",
    "    with open(synthetic_data_file, \"rb\") as f:\n",
    "        data_to_save = pickle.load(f)\n",
    "        queries = data_to_save['queries']\n",
    "        relevant_docs = data_to_save['relevant_docs']\n",
    "    print(f\"Loaded {len(queries)} existing questions.\")\n",
    "\n",
    "    processed_node_ids = set()\n",
    "    for q_id, node_ids in relevant_docs.items():\n",
    "        processed_node_ids.add(node_ids[0])\n",
    "    print(f\"Found {len(processed_node_ids)} already processed nodes.\")\n",
    "\n",
    "else:\n",
    "    print(\"No existing data file found. Starting from scratch.\")\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    processed_node_ids = set()\n",
    "\n",
    "# 2. Create the list of nodes that STILL need to be processed\n",
    "# (This logic is correct and remains the same)\n",
    "nodes_to_process = [\n",
    "    node for node in nodes if node.metadata[\"id\"] not in processed_node_ids\n",
    "]\n",
    "\n",
    "# 3. Run the generation only on the remaining nodes\n",
    "# (This logic is NEW and now loops in \"super-batches\")\n",
    "if not nodes_to_process:\n",
    "    print(\"All nodes have already been processed! Nothing to do.\")\n",
    "else:\n",
    "    print(f\"--- {len(nodes_to_process)} remaining nodes to process. ---\")\n",
    "    llm_qgen = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "\n",
    "    # We use a while loop to process in \"super-batches\"\n",
    "    while nodes_to_process:\n",
    "\n",
    "        # A. Get the next super-batch of nodes to process\n",
    "        current_batch_nodes = nodes_to_process[:SUPER_BATCH_SIZE]\n",
    "        print(f\"\\n--- Processing a new super-batch of {len(current_batch_nodes)} nodes... ---\")\n",
    "\n",
    "        try:\n",
    "            # B. Run the generation *only* on this small super-batch\n",
    "            # We pass the small INNER_BATCH_SIZE to prevent rate limits\n",
    "            new_queries, new_relevant_docs = generate_queries_batch(\n",
    "                current_batch_nodes,\n",
    "                llm_qgen,\n",
    "                num_questions_per_chunk=1, # Using 1 as in your last prompt\n",
    "                batch_size=INNER_BATCH_SIZE\n",
    "            )\n",
    "\n",
    "            # C. Add the new results to our main dataset\n",
    "            print(f\"Adding {len(new_queries)} new questions to our dataset...\")\n",
    "            queries.update(new_queries)\n",
    "            relevant_docs.update(new_relevant_docs)\n",
    "\n",
    "            # D. Save the COMBINED data back to the file (CRITICAL!)\n",
    "            # We do this *inside* the loop for crash-proof saving.\n",
    "            print(f\"Saving combined data ({len(queries)} total questions) to {synthetic_data_file}...\")\n",
    "            data_to_save = {'queries': queries, 'relevant_docs': relevant_docs}\n",
    "            with open(synthetic_data_file, \"wb\") as f:\n",
    "                pickle.dump(data_to_save, f)\n",
    "\n",
    "            # E. Update the list of nodes to process *in memory*\n",
    "            nodes_to_process = nodes_to_process[SUPER_BATCH_SIZE:]\n",
    "            print(f\"--- Super-batch complete. {len(nodes_to_process)} nodes remaining. ---\")\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            # If we hit a rate limit, print a message and wait 60 seconds.\n",
    "            # The 'continue' will retry this *same* super-batch.\n",
    "            print(f\"\\n!!! RATE LIMIT ERROR HIT !!!\")\n",
    "            print(\"Pausing for 60 seconds. The script will then retry this *same* batch.\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any other error happens, stop the script.\n",
    "            print(f\"\\n!!! AN UNEXPECTED ERROR OCCURRED !!!\")\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Stopping the script to prevent data loss.\")\n",
    "            print(f\"Progress up to the *previous* super-batch is saved in {synthetic_data_file}.\")\n",
    "            print(\"You can re-run this cell to resume from the last saved point.\")\n",
    "            break # Exit the while loop\n",
    "\n",
    "print(f\"\\n--- Process complete. Total questions in file: {len(queries)} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Showing 5 random pairs from 922 total generated pairs ---\n",
      "\n",
      "--- RANDOM PAIR 1 (from node 'node_316') ---\n",
      "GENERATED QUESTION:\n",
      "Can students with significant full-time work experience waive the Career Seminar\n",
      "course in the program?\n",
      "\n",
      "ORIGINAL CONTEXT (The 'Answer'):\n",
      "Course: Unknown Course Description: The Career Seminar (Pass/Fail) supports the\n",
      "development of industry professional skills, job and/or internship searches, and\n",
      "other in-demand areas of competency among today’s employers. Students enroll in\n",
      "the Career Seminar each quarter in order to engage in unique content throughout\n",
      "their degree program. Students with significant full-time work experience may be\n",
      "eligible to waive this course. 0 units, no cost. Details: • Career Seminar\n",
      "(Required) Pass/Fail The Career Seminar (Pass/Fail) supports the development of\n",
      "industry professional skills, job and/or internship searches, and other in-\n",
      "demand areas of competency among today’s employers. Students enroll in the\n",
      "Career Seminar each quarter in order to engage in unique content throughout\n",
      "their degree program. Students with significant full-time work experience may be\n",
      "eligible to waive this course. 0 units, no cost.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- RANDOM PAIR 2 (from node 'node_660') ---\n",
      "GENERATED QUESTION:\n",
      "What is the process for recommenders to submit their recommendation forms for\n",
      "the MS in Applied Data Science program at the University of Chicago?\n",
      "\n",
      "ORIGINAL CONTEXT (The 'Answer'):\n",
      "No. Recommendation forms and instructions are sent electronically to\n",
      "recommenders once their names are entered within the online application.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- RANDOM PAIR 3 (from node 'node_763') ---\n",
      "GENERATED QUESTION:\n",
      "What specific topics are covered in the Leadership and Consulting for Data\n",
      "Scientist course at the University of Chicago's MS in Applied Data Science\n",
      "program?\n",
      "\n",
      "ORIGINAL CONTEXT (The 'Answer'):\n",
      "The Leadership and Consulting for Data Scientist course is focused on:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- RANDOM PAIR 4 (from node 'node_820') ---\n",
      "GENERATED QUESTION:\n",
      "Can you provide more information on the eligibility criteria for the merit\n",
      "scholarships available for the MS in Applied Data Science program at the\n",
      "University of Chicago?\n",
      "\n",
      "ORIGINAL CONTEXT (The 'Answer'):\n",
      "Merit scholarships are available for eligible applicants. Once you apply to the\n",
      "program, you will be automatically considered for a scholarship. Early\n",
      "applications are highly encouraged.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- RANDOM PAIR 5 (from node 'node_562') ---\n",
      "GENERATED QUESTION:\n",
      "What specific courses or projects does Nick Kadochnikov teach or oversee in the\n",
      "University of Chicago Data Science Institute's MS in Applied Data Science\n",
      "program?\n",
      "\n",
      "ORIGINAL CONTEXT (The 'Answer'):\n",
      "Nick Kadochnikov is an Associate Clinical Professor at University of Chicago\n",
      "Data Science Institute as well as head of Artificial Intelligence and Advanced\n",
      "Technology group at Harbor Global, where he is at the forefront of creating\n",
      "cutting-edge Generative AI tools and advanced tech solutions. These innovative\n",
      "capabilities assist law firms and corporate legal teams in enhancing their\n",
      "service quality, making legal processes more streamlined, accessible, and\n",
      "requiring less manual effort.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "# 1. Load the data file you just generated\n",
    "synthetic_data_file = \"uchicago_synthetic_data_v2.pkl\"\n",
    "try:\n",
    "    with open(synthetic_data_file, \"rb\") as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        queries = loaded_data['queries']\n",
    "        relevant_docs = loaded_data['relevant_docs']\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {synthetic_data_file}.\")\n",
    "    print(\"Please make sure the Part 2 generation cell ran successfully.\")\n",
    "    queries = {} # Initialize to avoid error\n",
    "    relevant_docs = {} # Initialize to avoid error\n",
    "\n",
    "# 2. Re-create the node_id -> text content lookup\n",
    "# (The 'nodes' variable should still be in memory from Part 1)\n",
    "try:\n",
    "    node_id_to_content = {node.metadata[\"id\"]: node.page_content for node in nodes}\n",
    "except NameError:\n",
    "    print(\"Error: 'nodes' variable not found. Please re-run Part 1 to load the data.\")\n",
    "    node_id_to_content = {} # Initialize to avoid error\n",
    "\n",
    "# 3. Print 5 RANDOM generated Q&A pairs\n",
    "print(f\"--- Showing 5 random pairs from {len(queries)} total generated pairs ---\")\n",
    "\n",
    "if queries:\n",
    "    question_ids = list(queries.keys())\n",
    "\n",
    "    # Check if we have enough questions to sample 5, otherwise just show all\n",
    "    num_to_show = 5\n",
    "    if len(question_ids) < num_to_show:\n",
    "        random_q_ids = question_ids\n",
    "    else:\n",
    "        # --- THIS IS THE CHANGE ---\n",
    "        # Get 5 random question IDs\n",
    "        random_q_ids = random.sample(question_ids, num_to_show)\n",
    "\n",
    "    for i, q_id in enumerate(random_q_ids):\n",
    "        question = queries[q_id]\n",
    "        node_id = relevant_docs[q_id][0]\n",
    "        context = node_id_to_content.get(node_id, \"CONTEXT NOT FOUND\")\n",
    "\n",
    "        print(f\"\\n--- RANDOM PAIR {i+1} (from node '{node_id}') ---\")\n",
    "        print(f\"GENERATED QUESTION:\\n{textwrap.fill(question, 80)}\")\n",
    "        print(f\"\\nORIGINAL CONTEXT (The 'Answer'):\\n{textwrap.fill(context, 80)}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No queries found to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test JSON files already exist. Loading them.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4015426fbd104554abba39093285ecd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26b220118ae468dbfad9f04224f8960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 829\n",
      "Test dataset size: 93\n"
     ]
    }
   ],
   "source": [
    "train_dataset_file = \"uchicago_train_dataset_v2.json\"\n",
    "test_dataset_file = \"uchicago_test_dataset_v2.json\"\n",
    "\n",
    "if os.path.exists(train_dataset_file) and os.path.exists(test_dataset_file):\n",
    "    print(\"Train and test JSON files already exist. Loading them.\")\n",
    "    train_dataset = load_dataset(\"json\", data_files=train_dataset_file, split=\"train\")\n",
    "    test_dataset = load_dataset(\"json\", data_files=test_dataset_file, split=\"train\")\n",
    "else:\n",
    "    print(\"Creating and saving train/test JSON files...\")\n",
    "    # Create a lookup for node content\n",
    "    node_id_to_content = {node.metadata[\"id\"]: node.page_content for node in nodes}\n",
    "\n",
    "    prepared_data = []\n",
    "    for q_id, question in queries.items():\n",
    "        node_id = relevant_docs.get(q_id, [None])[0]\n",
    "        if node_id:\n",
    "            context = node_id_to_content.get(node_id)\n",
    "            if context:\n",
    "                prepared_data.append({\n",
    "                    \"question\": question,\n",
    "                    \"context\": context\n",
    "                })\n",
    "\n",
    "    # Convert to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_list(prepared_data)\n",
    "    # Rename columns to match sentence-transformers convention\n",
    "    dataset = dataset.rename_column(\"question\", \"anchor\")\n",
    "    dataset = dataset.rename_column(\"context\", \"positive\")\n",
    "    # Add a unique ID for each pair\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "    # Train/test split (90% train, 10% test)\n",
    "    dataset_split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    # Save datasets to JSON\n",
    "    dataset_split[\"train\"].to_json(train_dataset_file, orient=\"records\", lines=True)\n",
    "    dataset_split[\"test\"].to_json(test_dataset_file, orient=\"records\", lines=True)\n",
    "\n",
    "    train_dataset = dataset_split[\"train\"]\n",
    "    test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluator...\n",
      "Evaluator is ready.\n"
     ]
    }
   ],
   "source": [
    "# Define model ID and Matryoshka dimensions\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "matryoshka_dimensions = [384, 256, 128, 64]\n",
    "finetuned_model_output_dir = \"minilm-uchicago-msads-finetuned_v2\"\n",
    "\n",
    "print(\"Setting up evaluator...\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "corpus = dict(zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])) # All contexts\n",
    "queries_test = dict(zip(test_dataset[\"id\"], test_dataset[\"anchor\"])) # Test questions\n",
    "\n",
    "# Map test queries to their relevant contexts\n",
    "relevant_docs_test = {}\n",
    "for q_id in queries_test:\n",
    "    relevant_docs_test[q_id] = [q_id] # Query is relevant to its own positive context\n",
    "\n",
    "matryoshka_evaluators = []\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries_test,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs_test,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)\n",
    "print(\"Evaluator is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval initial model before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model (before fine-tuning)...\n",
      "\n",
      "--- Baseline Model Performance (ndcg@10) ---\n",
      "dim_384_cosine_ndcg@10: 0.2570\n",
      "dim_256_cosine_ndcg@10: 0.2345\n",
      "dim_128_cosine_ndcg@10: 0.2309\n",
      "dim_64_cosine_ndcg@10: 0.1768\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating baseline model (before fine-tuning)...\")\n",
    "base_model = SentenceTransformer(model_id)\n",
    "baseline_results = evaluator(base_model)\n",
    "\n",
    "print(\"\\n--- Baseline Model Performance (ndcg@10) ---\")\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print(f\"{key}: {baseline_results.get(key, 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring fine-tuning...\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfiguring fine-tuning...\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model for training\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"all-MiniLM-L6-v2-uchicago-msads-matryoshka_v2\", # New model name\n",
    "    ),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Safe Mode' Training Arguments to prevent RAM exhaustion.\n"
     ]
    }
   ],
   "source": [
    "# --- SAFE MODE FIX FOR LAPTOP FREEZING ---\n",
    "# This cell aggressively reduces memory usage for both\n",
    "# training and evaluation to prevent system-wide freezes.\n",
    "\n",
    "print(\"Using 'Safe Mode' Training Arguments to prevent RAM exhaustion.\")\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=finetuned_model_output_dir,\n",
    "    num_train_epochs=5,\n",
    "\n",
    "    # --- HERE IS THE FIX ---\n",
    "    per_device_train_batch_size=4,  # 1. Training fix: Use a tiny batch (was 16)\n",
    "    gradient_accumulation_steps=8,  # 2. Quality fix: Simulate a 16-size batch (2 * 8 = 16)\n",
    "    per_device_eval_batch_size=2,   # 3. Evaluator fix: Force evaluation to run in tiny batches\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",\n",
    "    logging_steps=25,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be6ec3909944dad9a1bebfb6d2acabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Fine-Tuning ---\n",
      "This will take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 05:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dim 384 Cosine Accuracy@1</th>\n",
       "      <th>Dim 384 Cosine Accuracy@3</th>\n",
       "      <th>Dim 384 Cosine Accuracy@5</th>\n",
       "      <th>Dim 384 Cosine Accuracy@10</th>\n",
       "      <th>Dim 384 Cosine Precision@1</th>\n",
       "      <th>Dim 384 Cosine Precision@3</th>\n",
       "      <th>Dim 384 Cosine Precision@5</th>\n",
       "      <th>Dim 384 Cosine Precision@10</th>\n",
       "      <th>Dim 384 Cosine Recall@1</th>\n",
       "      <th>Dim 384 Cosine Recall@3</th>\n",
       "      <th>Dim 384 Cosine Recall@5</th>\n",
       "      <th>Dim 384 Cosine Recall@10</th>\n",
       "      <th>Dim 384 Cosine Ndcg@10</th>\n",
       "      <th>Dim 384 Cosine Mrr@10</th>\n",
       "      <th>Dim 384 Cosine Map@100</th>\n",
       "      <th>Dim 256 Cosine Accuracy@1</th>\n",
       "      <th>Dim 256 Cosine Accuracy@3</th>\n",
       "      <th>Dim 256 Cosine Accuracy@5</th>\n",
       "      <th>Dim 256 Cosine Accuracy@10</th>\n",
       "      <th>Dim 256 Cosine Precision@1</th>\n",
       "      <th>Dim 256 Cosine Precision@3</th>\n",
       "      <th>Dim 256 Cosine Precision@5</th>\n",
       "      <th>Dim 256 Cosine Precision@10</th>\n",
       "      <th>Dim 256 Cosine Recall@1</th>\n",
       "      <th>Dim 256 Cosine Recall@3</th>\n",
       "      <th>Dim 256 Cosine Recall@5</th>\n",
       "      <th>Dim 256 Cosine Recall@10</th>\n",
       "      <th>Dim 256 Cosine Ndcg@10</th>\n",
       "      <th>Dim 256 Cosine Mrr@10</th>\n",
       "      <th>Dim 256 Cosine Map@100</th>\n",
       "      <th>Dim 128 Cosine Accuracy@1</th>\n",
       "      <th>Dim 128 Cosine Accuracy@3</th>\n",
       "      <th>Dim 128 Cosine Accuracy@5</th>\n",
       "      <th>Dim 128 Cosine Accuracy@10</th>\n",
       "      <th>Dim 128 Cosine Precision@1</th>\n",
       "      <th>Dim 128 Cosine Precision@3</th>\n",
       "      <th>Dim 128 Cosine Precision@5</th>\n",
       "      <th>Dim 128 Cosine Precision@10</th>\n",
       "      <th>Dim 128 Cosine Recall@1</th>\n",
       "      <th>Dim 128 Cosine Recall@3</th>\n",
       "      <th>Dim 128 Cosine Recall@5</th>\n",
       "      <th>Dim 128 Cosine Recall@10</th>\n",
       "      <th>Dim 128 Cosine Ndcg@10</th>\n",
       "      <th>Dim 128 Cosine Mrr@10</th>\n",
       "      <th>Dim 128 Cosine Map@100</th>\n",
       "      <th>Dim 64 Cosine Accuracy@1</th>\n",
       "      <th>Dim 64 Cosine Accuracy@3</th>\n",
       "      <th>Dim 64 Cosine Accuracy@5</th>\n",
       "      <th>Dim 64 Cosine Accuracy@10</th>\n",
       "      <th>Dim 64 Cosine Precision@1</th>\n",
       "      <th>Dim 64 Cosine Precision@3</th>\n",
       "      <th>Dim 64 Cosine Precision@5</th>\n",
       "      <th>Dim 64 Cosine Precision@10</th>\n",
       "      <th>Dim 64 Cosine Recall@1</th>\n",
       "      <th>Dim 64 Cosine Recall@3</th>\n",
       "      <th>Dim 64 Cosine Recall@5</th>\n",
       "      <th>Dim 64 Cosine Recall@10</th>\n",
       "      <th>Dim 64 Cosine Ndcg@10</th>\n",
       "      <th>Dim 64 Cosine Mrr@10</th>\n",
       "      <th>Dim 64 Cosine Map@100</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.877400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.146953</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.414863</td>\n",
       "      <td>0.365707</td>\n",
       "      <td>0.383556</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.143369</td>\n",
       "      <td>0.094624</td>\n",
       "      <td>0.055914</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.403086</td>\n",
       "      <td>0.353546</td>\n",
       "      <td>0.373586</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.146953</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.418961</td>\n",
       "      <td>0.357015</td>\n",
       "      <td>0.371233</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.372112</td>\n",
       "      <td>0.317405</td>\n",
       "      <td>0.334989</td>\n",
       "      <td>0.372112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.494624</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.058065</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.494624</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.408579</td>\n",
       "      <td>0.354583</td>\n",
       "      <td>0.373718</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.132616</td>\n",
       "      <td>0.094624</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.396730</td>\n",
       "      <td>0.342469</td>\n",
       "      <td>0.362716</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.132616</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>0.340199</td>\n",
       "      <td>0.357930</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>0.094624</td>\n",
       "      <td>0.058065</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.391408</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.349559</td>\n",
       "      <td>0.391408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.430337</td>\n",
       "      <td>0.379254</td>\n",
       "      <td>0.396830</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.060215</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.418645</td>\n",
       "      <td>0.360970</td>\n",
       "      <td>0.378207</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.146953</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>0.060215</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.425608</td>\n",
       "      <td>0.369265</td>\n",
       "      <td>0.385283</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.062366</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.424737</td>\n",
       "      <td>0.363539</td>\n",
       "      <td>0.377332</td>\n",
       "      <td>0.424737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.062366</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.388245</td>\n",
       "      <td>0.406184</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.143369</td>\n",
       "      <td>0.103226</td>\n",
       "      <td>0.063441</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.435653</td>\n",
       "      <td>0.373622</td>\n",
       "      <td>0.390762</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.146953</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.063441</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.435776</td>\n",
       "      <td>0.373088</td>\n",
       "      <td>0.388734</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.411514</td>\n",
       "      <td>0.349441</td>\n",
       "      <td>0.363992</td>\n",
       "      <td>0.411514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.060215</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.437221</td>\n",
       "      <td>0.385514</td>\n",
       "      <td>0.404241</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>0.062366</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.439854</td>\n",
       "      <td>0.382079</td>\n",
       "      <td>0.398815</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.143369</td>\n",
       "      <td>0.105376</td>\n",
       "      <td>0.058065</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.411766</td>\n",
       "      <td>0.357258</td>\n",
       "      <td>0.376722</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.494624</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.132616</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.494624</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.420558</td>\n",
       "      <td>0.361137</td>\n",
       "      <td>0.375399</td>\n",
       "      <td>0.420558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Complete ---\n",
      "Fine-tuned model saved to minilm-uchicago-msads-finetuned_v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    # Note: MNRL loss expects columns in order: anchor, positive\n",
    "    train_dataset=train_dataset.select_columns([\"anchor\", \"positive\"]),\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"--- Starting Model Fine-Tuning ---\")\n",
    "print(\"This will take some time...\")\n",
    "trainer.train()\n",
    "print(\"--- Training Complete ---\")\n",
    "trainer.save_model()\n",
    "print(f\"Fine-tuned model saved to {finetuned_model_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating fine-tuned model...\n",
      "\n",
      "--- Fine-Tuned Model Performance (ndcg@10) ---\n",
      "dim_384_cosine_ndcg@10: 0.4440\n",
      "dim_256_cosine_ndcg@10: 0.4357\n",
      "dim_128_cosine_ndcg@10: 0.4358\n",
      "dim_64_cosine_ndcg@10: 0.4115\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=device\n",
    ")\n",
    "finetuned_results = evaluator(fine_tuned_model)\n",
    "\n",
    "print(\"\\n--- Fine-Tuned Model Performance (ndcg@10) ---\")\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print(f\"{key}: {finetuned_results.get(key, 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this fintuned model has better performance than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RAG with newly trained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned embeddings from minilm-uchicago-msads-finetuned_v2...\n",
      "Filtering complex metadata from nodes...\n",
      "Creating and saving vector store to: chroma_db_finetuned...\n",
      "Vector store built and saved locally.\n"
     ]
    }
   ],
   "source": [
    "# --- REPLACEMENT FOR NOTEBOOK CELL 24 ---\n",
    "\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Define the path where the database will be saved\n",
    "DB_PERSIST_PATH = \"chroma_db_finetuned\"\n",
    "\n",
    "print(f\"Loading fine-tuned embeddings from {finetuned_model_output_dir}...\")\n",
    "\n",
    "# Set device for embeddings\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=finetuned_model_output_dir,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"Filtering complex metadata from nodes...\")\n",
    "filtered_nodes = filter_complex_metadata(nodes)\n",
    "\n",
    "print(f\"Creating and saving vector store to: {DB_PERSIST_PATH}...\")\n",
    "# This command creates the DB and saves it to the persist_directory\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_nodes,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=DB_PERSIST_PATH  # <-- This is the new line\n",
    ")\n",
    "\n",
    "print(\"Vector store built and saved locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever is ready with k=10.\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THE FAST, EXPERIMENTAL CELL\n",
    "k_value = 10 # <-- Change this value to 5, 8, 10, etc.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": k_value})\n",
    "print(f\"Retriever is ready with k={k_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Rag Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain with typo correction is built and ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Define LLMs\n",
    "# The \"GPT LLM\" (gpt-4o) for final answer generation\n",
    "llm_rag = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "# The LLM for correcting user typos\n",
    "llm_correction = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 2. Define the Document Formatter (from your notebook)\n",
    "# It's designed to read the exact JSON structure you provided\n",
    "def format_docs_with_links(docs):\n",
    "    \"\"\"\n",
    "    Formats retrieved documents to include their source URL and\n",
    "    any embedded links found in the metadata.\n",
    "    \"\"\"\n",
    "    formatted_chunks = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # Get the source URL\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "\n",
    "        # --- THIS PART HANDLES YOUR 'embedded_links' ---\n",
    "        links = doc.metadata.get('embedded_links')\n",
    "        links_string = \"\"  # Start with an empty string\n",
    "\n",
    "        if links:  # Check if links is not None and not empty\n",
    "            link_parts = []\n",
    "            for link in links:\n",
    "                text = link.get('text', 'link')\n",
    "                href = link.get('href', '#')\n",
    "                # Format as a Markdown link, which the LLM can understand\n",
    "                link_parts.append(f\"• [{text}]({href})\")\n",
    "\n",
    "            # Create a clearly labeled section for the LLM\n",
    "            links_string = \"\\n\\n**Embedded Links Found:**\\n\" + \"\\n\".join(link_parts)\n",
    "        # --- END OF 'embedded_links' HANDLER ---\n",
    "\n",
    "        # Create a formatted block for each chunk\n",
    "        chunk_with_source = (\n",
    "            f\"--- Context Chunk {i+1} (Source: {source}) ---\\n\"\n",
    "            f\"{doc.page_content}\"\n",
    "            f\"{links_string}\"  # Append the new links string\n",
    "        )\n",
    "        formatted_chunks.append(chunk_with_source)\n",
    "\n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "# 3. Define the Main RAG Prompt (from your notebook)\n",
    "# --- SYNTAX ERROR IS FIXED HERE ---\n",
    "system_prompt = (\n",
    "    \"You are an expert assistant for the University of Chicago's MS in \"\n",
    "    \"Applied Data Science program. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"Each context chunk includes its source URL and may also include a list \"\n",
    "    \"of 'Embedded Links Found'.\\n\\n\"\n",
    "\n",
    "    \"**Critical Rules for Answering:**\\n\"\n",
    "    \"1.  **Ground your answer in the context.** Quote or paraphrase the provided text. \"\n",
    "    \"2.  **If the context chunk you use has an 'Embedded Links Found' section,** \"\n",
    "    \"you MUST include the relevant link(s) in your answer. \"\n",
    "    \"Format them as [Link Text](URL).\\n\"\n",
    "    \"3.  If your answer is a fact (like a tuition number), you do not need a link \"\n",
    "    \"unless one is explicitly provided in the 'Embedded Links Found' section.\\n\"\n",
    "    \"4.  If you don't know the answer from the context, just say that you don't know.\\n\\n\"\n",
    "\n",
    "    \"**Example Answer:**\\n\"\n",
    "    \"For questions regarding an application fee waiver, you should refer to \"\n",
    "    \"the [Physical Sciences Division fee waiver policy](https://...#FeeWaiver).\\n\\n\"\n",
    "\n",
    "    \"Context:\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "# 4. Define the Final Answer Chain\n",
    "question_answer_chain = (\n",
    "    prompt\n",
    "    | llm_rag\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. --- NEW: Define the Query Correction Chain (from Streamlit) ---\n",
    "correction_prompt_template = (\n",
    "    \"You are an expert query assistant. Your job is to correct any typos in a \"\n",
    "    \"user's query. Do not answer the question, just output the corrected query. \"\n",
    "    \"User query: {question}\"\n",
    ")\n",
    "correction_prompt = ChatPromptTemplate.from_template(correction_prompt_template)\n",
    "\n",
    "# This chain takes a string query and outputs a corrected string query\n",
    "correction_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | correction_prompt\n",
    "    | llm_correction\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 6. --- NEW: Define a chain to get context using the corrected query ---\n",
    "# This chain will:\n",
    "# 1. Take the original query string\n",
    "# 2. Run the correction_chain to get a corrected query\n",
    "# 3. Use the corrected query to search the retriever\n",
    "# 4. Format the retrieved docs\n",
    "context_retrieval_chain = (\n",
    "    RunnableParallel(\n",
    "        corrected_question=correction_chain,\n",
    "        original_question=RunnablePassthrough() # We pass this just for the lambda\n",
    "    )\n",
    "    | (lambda x: x[\"corrected_question\"]) # Pass only the corrected query string\n",
    "    | retriever\n",
    "    | format_docs_with_links\n",
    ")\n",
    "\n",
    "# 7. --- UPDATED: Define the Main RAG Chain ---\n",
    "# This chain now uses the context_retrieval_chain (which has typo correction)\n",
    "# It still passes the ORIGINAL user query to the final prompt\n",
    "rag_chain = RunnableParallel(\n",
    "    context=context_retrieval_chain,\n",
    "    question=RunnablePassthrough() # Pass the original, uncorrected query\n",
    ").assign(answer=question_answer_chain)\n",
    "\n",
    "print(\"RAG chain with typo correction is built and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Comprehensive Evaluation Queries ---\n",
      "Running 12 evaluation queries...\n",
      "You can compare the 'Answer:' from the model to your sample answers to see how well it's working.\n",
      "\n",
      "Query: What is tuition cost for the program?\n",
      "Answer:\n",
      "The tuition for the MS in Applied Data Science program at the University of\n",
      "Chicago is $6,384 per course. For the 12-course program, the total tuition is\n",
      "$76,608, and for the 18-course program, it is $114,912. Additionally, there is a\n",
      "non-refundable program enrollment deposit of $1,500, which is credited toward\n",
      "your first quarter’s tuition balance.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What scholarships are available for the program?\n",
      "Answer:\n",
      "The MS in Applied Data Science program at the University of Chicago offers\n",
      "partial tuition merit-based scholarships to top applicants. These scholarships\n",
      "do not require a separate application, but it is recommended that candidates\n",
      "submit their applications ahead of the early deadline to maximize their chances\n",
      "of securing a scholarship. Additionally, students are encouraged to investigate\n",
      "scholarships offered through various civic and professional organizations,\n",
      "foundations, and state agencies. For more information, you can refer to the\n",
      "[Tuition, Fees, & Aid page](https://datascience.uchicago.edu/education/masters-\n",
      "programs/ms-in-applied-data-science/tuition-fees-aid/).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What are the minimum scores for the TOEFL and IELTS English Language Requirement?\n",
      "Answer:\n",
      "The minimum scores for the Master’s in Applied Data Science program at the\n",
      "University of Chicago are as follows: a TOEFL score of 102 (with no subscore\n",
      "requirement) and an IELTS score of 7 (with no subscore requirement) as mentioned\n",
      "in [Context Chunk 1](https://datascience.uchicago.edu/education/masters-\n",
      "programs/ms-in-applied-data-science/how-to-apply/).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Is there an application fee waiver?\n",
      "Answer:\n",
      "For questions regarding an application fee waiver, you should refer to the\n",
      "[Physical Sciences Division fee waiver\n",
      "policy](https://datascience.uchicago.edu/education/masters-programs/ms-in-\n",
      "applied-data-science/how-to-apply/).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What are the deadlines for the in-person program?\n",
      "Answer:\n",
      "The final application deadline for the in-person program is June 23, 2026.\n",
      "Admissions decisions for the in-person program are typically released 1-2 months\n",
      "after each application deadline, and your application must be complete to be\n",
      "considered for review.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: How long will it take for me to receive a decision on my application?\n",
      "Answer:\n",
      "Admissions decisions for the Master's in Applied Data Science program are\n",
      "typically released 1-2 months after each application deadline. Only completed\n",
      "applications are reviewed.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Can I set up an advising appointment with the enrollment management team?\n",
      "Answer:\n",
      "Yes, you can set up an advising appointment with the enrollment management team.\n",
      "For the Online Master of Science in Applied Data Science program, you can\n",
      "schedule an appointment with Patrick Vonesh, the Senior Assistant Director of\n",
      "Enrollment Management. For the In-Person Master of Science in Applied Data\n",
      "Science program, you can schedule an appointment with Jose Alvarado, the\n",
      "Associate Director of Enrollment Management. You can find more information and\n",
      "start your application or schedule an appointment\n",
      "[here](https://datascience.uchicago.edu/education/masters-programs/ms-in-\n",
      "applied-data-science/how-to-apply/).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Where can I mail my official transcripts?\n",
      "Answer:\n",
      "If your institution cannot send your documents electronically, you should have\n",
      "them send your official transcripts to the following mailing address:  The\n",
      "University of Chicago   Attention: MS in Applied Data Science Admissions   455 N\n",
      "Cityfront Plaza Dr., Suite 2800   Chicago, Illinois 60611\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Does the Master’s in Applied Data Science Online program provide visa sponsorship?\n",
      "Answer:\n",
      "No, the Master’s in Applied Data Science Online program does not provide visa\n",
      "sponsorship. International students are welcome to apply, but the program is not\n",
      "eligible for visa sponsorship.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: How do I apply to the MBA/MS program?\n",
      "Answer:\n",
      "To apply to the MBA/MS joint degree program, you need to go through Booth’s\n",
      "centralized, joint-application process. You should complete the Chicago Booth\n",
      "Full-Time MBA application and select the MBA/MS in Applied Data Science as your\n",
      "program of interest. An MBA/MS program supplement, which contains Applied Data\n",
      "Science specific questions, will be available for completion within your Booth\n",
      "application. This supplement will be reviewed by the Applied Data Science\n",
      "admissions team along with your full Booth application. For complete\n",
      "consideration, you should complete both the MBA application and the joint degree\n",
      "program supplement in the same application round before submitting the\n",
      "application.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Is the MS in Applied Data Science program STEM/OPT eligible?\n",
      "Answer:\n",
      "Yes, the full-time, In-Person Master’s in Applied Data Science program at the\n",
      "University of Chicago is listed as a STEM-designated degree by the U.S.\n",
      "Department of Homeland Security for the purposes of the STEM OPT extension,\n",
      "allowing eligible students to apply. However, approval of STEM OPT is at the\n",
      "discretion of U.S. Citizenship & Immigration Services.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: How many courses must you complete to earn UChicago’s Master’s in Applied Data Science?\n",
      "Answer:\n",
      "To earn UChicago’s Master’s in Applied Data Science, you must successfully\n",
      "complete 12 courses, which include 6 core courses, 4 elective courses, and 2\n",
      "Capstone courses, along with a tailored Career Seminar. There is also an option\n",
      "for an 18-course thesis track, which includes additional electives and a thesis\n",
      "requirement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- RAG Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"\\n--- Running Comprehensive Evaluation Queries ---\")\n",
    "\n",
    "# This is the new, much better list of questions you provided\n",
    "evaluation_queries = [\n",
    "    \"What is tuition cost for the program?\",\n",
    "    \"What scholarships are available for the program?\",\n",
    "    \"What are the minimum scores for the TOEFL and IELTS English Language Requirement?\",\n",
    "    \"Is there an application fee waiver?\",\n",
    "    \"What are the deadlines for the in-person program?\",\n",
    "    \"How long will it take for me to receive a decision on my application?\",\n",
    "    \"Can I set up an advising appointment with the enrollment management team?\",\n",
    "    \"Where can I mail my official transcripts?\",\n",
    "    \"Does the Master’s in Applied Data Science Online program provide visa sponsorship?\",\n",
    "    \"How do I apply to the MBA/MS program?\",\n",
    "    \"Is the MS in Applied Data Science program STEM/OPT eligible?\",\n",
    "    \"How many courses must you complete to earn UChicago’s Master’s in Applied Data Science?\"\n",
    "]\n",
    "\n",
    "print(f\"Running {len(evaluation_queries)} evaluation queries...\")\n",
    "print(\"You can compare the 'Answer:' from the model to your sample answers to see how well it's working.\")\n",
    "\n",
    "for query in evaluation_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "\n",
    "    # Invoke the RAG chain\n",
    "    response = rag_chain.invoke(query)\n",
    "\n",
    "    # Print the model's generated answer\n",
    "    print(f\"Answer:\\n{textwrap.fill(response['answer'], 80)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n--- RAG Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
